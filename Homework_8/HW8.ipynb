{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 8 - Topic Modeling**\n",
    "# KDD Tuesdays 12:30 PM - 2:45 PM\n",
    "## Jake Brulato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relvant packages for conduct topic modeling analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "np.random.seed(2023)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defined Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import*\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read all the text files in the directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16748.txt',\n",
       " '17108.txt',\n",
       " '17109.txt',\n",
       " '17110.txt',\n",
       " '17111.txt',\n",
       " '17116.txt',\n",
       " '17117.txt',\n",
       " '17118.txt',\n",
       " '17119.txt',\n",
       " '17139.txt',\n",
       " '17144.txt',\n",
       " '17145.txt',\n",
       " '17146.txt',\n",
       " '17147.txt',\n",
       " '17150.txt',\n",
       " '17185.txt',\n",
       " '17192.txt',\n",
       " '17219.txt',\n",
       " '17239.txt',\n",
       " '17243.txt',\n",
       " '17254.txt',\n",
       " '17255.txt',\n",
       " '17280.txt',\n",
       " '17300.txt',\n",
       " '17303.txt',\n",
       " '17341.txt',\n",
       " '17384.txt',\n",
       " '17391.txt',\n",
       " '17398.txt',\n",
       " '17399.txt',\n",
       " '17430.txt',\n",
       " '17431.txt',\n",
       " '17447.txt',\n",
       " '17457.txt',\n",
       " '17460.txt',\n",
       " '17501.txt',\n",
       " '17518.txt',\n",
       " '17532.txt',\n",
       " '17534.txt',\n",
       " '17578.txt',\n",
       " '17609.txt',\n",
       " '17610.txt',\n",
       " '17655.txt',\n",
       " '17662.txt',\n",
       " '17663.txt',\n",
       " '17695.txt',\n",
       " '17711.txt',\n",
       " '17713.txt',\n",
       " '17753.txt',\n",
       " '17757.txt',\n",
       " '17758.txt',\n",
       " '17761.txt',\n",
       " '17803.txt',\n",
       " '17811.txt',\n",
       " '17874.txt',\n",
       " '17879.txt',\n",
       " '17886.txt',\n",
       " '17896.txt',\n",
       " '17898.txt',\n",
       " '17902.txt',\n",
       " '17912.txt',\n",
       " '17933.txt',\n",
       " '17934.txt',\n",
       " '17945.txt',\n",
       " '17963.txt',\n",
       " '17971.txt',\n",
       " '17992.txt',\n",
       " '18004.txt',\n",
       " '18016.txt',\n",
       " '18032.txt',\n",
       " '18067.txt',\n",
       " '18068.txt',\n",
       " '18080.txt',\n",
       " '18087.txt',\n",
       " '18088.txt',\n",
       " '18136.txt',\n",
       " '18141.txt',\n",
       " '18156.txt',\n",
       " '18161.txt',\n",
       " '18181.txt',\n",
       " '18227.txt',\n",
       " '18263.txt',\n",
       " '18272.txt',\n",
       " '18273.txt',\n",
       " '18274.txt',\n",
       " '18282.txt',\n",
       " '18283.txt',\n",
       " '18307.txt',\n",
       " '18368.txt',\n",
       " '18375.txt',\n",
       " '18376.txt',\n",
       " '18396.txt',\n",
       " '18406.txt',\n",
       " '18413.txt',\n",
       " '18414.txt',\n",
       " '18447.txt',\n",
       " '18473.txt',\n",
       " '18480.txt',\n",
       " '18485.txt',\n",
       " '18498.txt',\n",
       " '1858.txt',\n",
       " '1859.txt',\n",
       " '1860.txt',\n",
       " '1864.txt',\n",
       " '1865.txt',\n",
       " '1866.txt',\n",
       " '1867.txt',\n",
       " '1889.txt',\n",
       " '1891.txt',\n",
       " '1908.txt',\n",
       " '1910.txt',\n",
       " '1911.txt',\n",
       " '1912.txt',\n",
       " '1916.txt',\n",
       " '1917.txt',\n",
       " '1921.txt',\n",
       " '1925.txt',\n",
       " '1928.txt',\n",
       " '1929.txt',\n",
       " '1930.txt',\n",
       " '1932.txt',\n",
       " '1934.txt',\n",
       " '1937.txt',\n",
       " '1943.txt',\n",
       " '1944.txt',\n",
       " '1945.txt',\n",
       " '1961.txt',\n",
       " '1967.txt',\n",
       " '1968.txt',\n",
       " '1974.txt',\n",
       " '1975.txt',\n",
       " '1976.txt',\n",
       " '1979.txt',\n",
       " '1981.txt',\n",
       " '1984.txt',\n",
       " '1985.txt',\n",
       " '1990.txt',\n",
       " '1991.txt',\n",
       " '1994.txt',\n",
       " '1998.txt',\n",
       " '2005.txt',\n",
       " '2006.txt',\n",
       " '2007.txt',\n",
       " '2008.txt',\n",
       " '2009.txt',\n",
       " '2025.txt',\n",
       " '2026.txt',\n",
       " '2030.txt',\n",
       " '2031.txt',\n",
       " '2033.txt',\n",
       " '2035.txt',\n",
       " '2036.txt',\n",
       " '2043.txt',\n",
       " '2045.txt',\n",
       " '2046.txt',\n",
       " '2051.txt',\n",
       " '2055.txt',\n",
       " '2058.txt',\n",
       " '2059.txt',\n",
       " '2062.txt',\n",
       " '2067.txt',\n",
       " '2076.txt',\n",
       " '2079.txt',\n",
       " '2080.txt',\n",
       " '2081.txt',\n",
       " '2085.txt',\n",
       " '2086.txt',\n",
       " '2087.txt',\n",
       " '2089.txt',\n",
       " '2090.txt',\n",
       " '2091.txt',\n",
       " '2094.txt',\n",
       " '2095.txt',\n",
       " '2098.txt',\n",
       " '2099.txt',\n",
       " '2101.txt',\n",
       " '2108.txt',\n",
       " '2110.txt',\n",
       " '2113.txt',\n",
       " '2115.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'MovieReviews' # Folder Name\n",
    "file_names = PlaintextCorpusReader(corpus_root, '.*',encoding='latin-1')  # wildcard is read all files in the folder\n",
    "file_names.fileids()  # Get the filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Read all the text files in the directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = 'C:/Users/rbrul/Documents/GitHub/KDD/Homework_8/MovieReviews'\n",
    "# path: str = '/Users/jakebrulato/Documents/GitHub/KDD/Homework_8/MovieReviews'\n",
    "file_names = os.listdir(path)\n",
    "\n",
    "doc_contents: list = []\n",
    "for i, file_name in zip(range(len(file_names)), file_names):\n",
    "    with open(path + '/' + file_name, encoding=\"utf8\", errors='ignore') as file:\n",
    "        doc_contents.append((i, file_name, file.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build dictionary out of the document contents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNum   FileName                                        FileContent\n",
      "0       0  16748.txt  DENNIS SCHWARTZ \"Movie Reviews and Poetry\"\\nUN...\n",
      "1       1  17108.txt  A brilliant, witty mock documentary of Jean Se...\n",
      "2       2  17109.txt  NOSTALGHIA (director: Andrei Tarkovsky; cast: ...\n",
      "3       3  17110.txt  PAYBACK (director: Brian Helgeland; cast:(Port...\n",
      "4       4  17111.txt  WAKING NED DEVINE (director: Kirk Jones (III);...\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(doc_contents, columns=['RowNum', 'FileName', 'FileContent'])\n",
    "data.dropna(subset=['FileContent'], inplace= True)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load documents contenets into a dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rbrul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [denni, schwartz, movi, review, poetri, unmak,...\n",
      "1    [brilliant, witti, mock, documentari, jean, se...\n",
      "2    [nostalghia, director, andrei, tarkovski, cast...\n",
      "3    [payback, director, brian, helgeland, cast, po...\n",
      "4    [wake, devin, director, kirk, jone, cast, bann...\n",
      "Name: FileContent, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "processed_docs = data['FileContent'].map(preprocess)\n",
    "print(processed_docs[:5])\n",
    "\n",
    "dictionary = Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build BagOfWords corpus from the document content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bag of Words test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"accomplish\") appears 1 times.\n",
      "Word 1 (\"actor\") appears 1 times.\n",
      "Word 2 (\"america\") appears 1 times.\n",
      "Word 3 (\"apart\") appears 1 times.\n",
      "Word 4 (\"attract\") appears 3 times.\n"
     ]
    }
   ],
   "source": [
    "test_corpus = bow_corpus[0]\n",
    "for i, word_stat in zip(range(len(test_corpus)), test_corpus):\n",
    "    # print only the first 5 words\n",
    "    if i < 5:\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} times.\".format(word_stat[0], dictionary[word_stat[0]], word_stat[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Build LDA model With 10 Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 -> Words: 0.014*\"money\" + 0.010*\"school\" + 0.009*\"problem\" + 0.008*\"expect\" + 0.007*\"friend\" + 0.007*\"origin\" + 0.007*\"tri\" + 0.007*\"place\" + 0.006*\"role\" + 0.006*\"start\"\n",
      "Topic: 1 -> Words: 0.009*\"paul\" + 0.009*\"artist\" + 0.008*\"see\" + 0.008*\"feel\" + 0.007*\"perform\" + 0.007*\"show\" + 0.006*\"mark\" + 0.006*\"need\" + 0.005*\"friend\" + 0.005*\"tri\"\n",
      "Topic: 2 -> Words: 0.011*\"littl\" + 0.009*\"book\" + 0.009*\"screen\" + 0.008*\"long\" + 0.008*\"chang\" + 0.008*\"perform\" + 0.007*\"novel\" + 0.007*\"plot\" + 0.006*\"action\" + 0.006*\"john\"\n",
      "Topic: 3 -> Words: 0.010*\"releas\" + 0.010*\"littl\" + 0.009*\"music\" + 0.008*\"audienc\" + 0.007*\"action\" + 0.007*\"produc\" + 0.007*\"money\" + 0.006*\"michael\" + 0.006*\"turn\" + 0.005*\"reason\"\n",
      "Topic: 4 -> Words: 0.007*\"lover\" + 0.007*\"polit\" + 0.007*\"show\" + 0.007*\"romanc\" + 0.007*\"littl\" + 0.006*\"person\" + 0.006*\"singl\" + 0.006*\"home\" + 0.006*\"have\" + 0.006*\"murder\"\n",
      "Topic: 5 -> Words: 0.009*\"world\" + 0.008*\"famili\" + 0.008*\"littl\" + 0.007*\"father\" + 0.007*\"best\" + 0.006*\"problem\" + 0.006*\"feel\" + 0.006*\"place\" + 0.006*\"town\" + 0.006*\"see\"\n",
      "Topic: 6 -> Words: 0.014*\"wife\" + 0.008*\"act\" + 0.008*\"father\" + 0.008*\"feel\" + 0.008*\"real\" + 0.008*\"audienc\" + 0.008*\"famili\" + 0.007*\"hard\" + 0.007*\"comedi\" + 0.007*\"turn\"\n",
      "Topic: 7 -> Words: 0.012*\"action\" + 0.008*\"compani\" + 0.007*\"screen\" + 0.007*\"better\" + 0.007*\"question\" + 0.007*\"begin\" + 0.006*\"line\" + 0.006*\"tri\" + 0.006*\"kill\" + 0.006*\"real\"\n",
      "Topic: 8 -> Words: 0.012*\"world\" + 0.011*\"feel\" + 0.008*\"talk\" + 0.008*\"women\" + 0.008*\"role\" + 0.007*\"great\" + 0.007*\"tri\" + 0.007*\"see\" + 0.007*\"music\" + 0.006*\"relationship\"\n",
      "Topic: 9 -> Words: 0.009*\"jack\" + 0.009*\"need\" + 0.008*\"question\" + 0.008*\"tri\" + 0.008*\"see\" + 0.007*\"believ\" + 0.007*\"real\" + 0.006*\"action\" + 0.006*\"feel\" + 0.006*\"turn\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)\n",
    "\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} -> Words: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Top 10 Words for each topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 -> Top Words: [(0.009110279, 'jack'), (0.008815186, 'need'), (0.008435835, 'question'), (0.0076961187, 'tri'), (0.007628205, 'see'), (0.006812147, 'believ'), (0.006757227, 'real'), (0.006111999, 'action'), (0.0060606995, 'feel'), (0.006007181, 'turn')]\n",
      "Topic: 1 -> Top Words: [(0.011611248, 'world'), (0.011028524, 'feel'), (0.008025016, 'talk'), (0.007964426, 'women'), (0.0076935138, 'role'), (0.0072711306, 'great'), (0.007047095, 'tri'), (0.0070455396, 'see'), (0.0065199705, 'music'), (0.0063639693, 'relationship')]\n",
      "Topic: 2 -> Top Words: [(0.009827383, 'releas'), (0.009562484, 'littl'), (0.009049662, 'music'), (0.0076763663, 'audienc'), (0.0071927644, 'action'), (0.006645522, 'produc'), (0.0065885372, 'money'), (0.006281782, 'michael'), (0.0059787543, 'turn'), (0.0053524706, 'reason')]\n",
      "Topic: 3 -> Top Words: [(0.011665505, 'action'), (0.0082615595, 'compani'), (0.007038567, 'screen'), (0.0066807275, 'better'), (0.0065927957, 'question'), (0.0065694875, 'begin'), (0.0061623203, 'line'), (0.0059806877, 'tri'), (0.005854503, 'kill'), (0.0057804724, 'real')]\n",
      "Topic: 4 -> Top Words: [(0.0088264365, 'world'), (0.00826124, 'famili'), (0.007689505, 'littl'), (0.007151309, 'father'), (0.0069009974, 'best'), (0.0062456396, 'problem'), (0.00608959, 'feel'), (0.005923989, 'place'), (0.005783864, 'town'), (0.00563983, 'see')]\n",
      "Topic: 5 -> Top Words: [(0.009239242, 'paul'), (0.008901378, 'artist'), (0.008418461, 'see'), (0.008104552, 'feel'), (0.0073435144, 'perform'), (0.0067203557, 'show'), (0.0063215746, 'mark'), (0.0057413303, 'need'), (0.005450384, 'friend'), (0.005360517, 'tri')]\n",
      "Topic: 6 -> Top Words: [(0.013852209, 'money'), (0.010357336, 'school'), (0.008949036, 'problem'), (0.007532326, 'expect'), (0.007264283, 'friend'), (0.007059962, 'origin'), (0.006889379, 'tri'), (0.00669653, 'place'), (0.006263493, 'role'), (0.005782968, 'start')]\n",
      "Topic: 7 -> Top Words: [(0.0139755495, 'wife'), (0.007926105, 'act'), (0.007905762, 'father'), (0.007752468, 'feel'), (0.0077316854, 'real'), (0.0076857773, 'audienc'), (0.007624102, 'famili'), (0.007332527, 'hard'), (0.007001363, 'comedi'), (0.0066744117, 'turn')]\n",
      "Topic: 8 -> Top Words: [(0.010845949, 'littl'), (0.009477583, 'book'), (0.00925984, 'screen'), (0.008309041, 'long'), (0.008133497, 'chang'), (0.007595186, 'perform'), (0.007339466, 'novel'), (0.006742693, 'plot'), (0.0064129564, 'action'), (0.0057404856, 'john')]\n",
      "Topic: 9 -> Top Words: [(0.00725575, 'lover'), (0.007243366, 'polit'), (0.0066846805, 'show'), (0.0066192085, 'romanc'), (0.006577224, 'littl'), (0.006436152, 'person'), (0.006428052, 'singl'), (0.0059410874, 'home'), (0.0056594014, 'have'), (0.0055883625, 'murder')]\n"
     ]
    }
   ],
   "source": [
    "top_topics = lda_model.top_topics(corpus=bow_corpus, topn=10)\n",
    "\n",
    "i = 0\n",
    "for words, coherence in top_topics:\n",
    "    print('Topic: {} -> Top Words: {}'.format(i, words))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Document -> Topic probabilities for all the documents in the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - 16748.txt -> [(4, 0.6654528), (5, 0.26274422), (8, 0.066289455)]\n",
      "1 - 17108.txt -> [(5, 0.17996767), (8, 0.19980069), (9, 0.61419517)]\n",
      "2 - 17109.txt -> [(5, 0.9409828), (8, 0.055379685)]\n",
      "3 - 17110.txt -> [(3, 0.61053795), (8, 0.018494712), (9, 0.36647862)]\n",
      "4 - 17111.txt -> [(3, 0.21550938), (5, 0.7786917)]\n",
      "5 - 17116.txt -> [(5, 0.34349066), (6, 0.48683968), (8, 0.12873161), (9, 0.03813343)]\n",
      "6 - 17117.txt -> [(5, 0.1629894), (8, 0.61665076), (9, 0.21399423)]\n",
      "7 - 17118.txt -> [(1, 0.514556), (8, 0.47719425)]\n",
      "8 - 17119.txt -> [(9, 0.99552095)]\n",
      "9 - 17139.txt -> [(5, 0.9939169)]\n",
      "10 - 17144.txt -> [(5, 0.2878855), (6, 0.25209427), (8, 0.4563541)]\n",
      "11 - 17145.txt -> [(1, 0.9903196)]\n",
      "12 - 17146.txt -> [(5, 0.7485997), (8, 0.2471436)]\n",
      "13 - 17147.txt -> [(1, 0.988747)]\n",
      "14 - 17150.txt -> [(1, 0.76285326), (8, 0.18677412), (9, 0.04579607)]\n",
      "15 - 17185.txt -> [(5, 0.025522908), (6, 0.11788307), (9, 0.8516276)]\n",
      "16 - 17192.txt -> [(8, 0.99125963)]\n",
      "17 - 17219.txt -> [(0, 0.35930455), (1, 0.42927274), (3, 0.20288348)]\n",
      "18 - 17239.txt -> [(0, 0.28822544), (5, 0.6974852)]\n",
      "19 - 17243.txt -> [(1, 0.21025994), (5, 0.68117356), (6, 0.101767994)]\n",
      "20 - 17254.txt -> [(1, 0.5612274), (6, 0.08181553), (9, 0.34950793)]\n",
      "21 - 17255.txt -> [(6, 0.9907186)]\n",
      "22 - 17280.txt -> [(1, 0.9057343), (9, 0.088424295)]\n",
      "23 - 17300.txt -> [(5, 0.47583863), (6, 0.2923085), (9, 0.22450578)]\n",
      "24 - 17303.txt -> [(1, 0.044684358), (2, 0.4255853), (5, 0.4402289), (8, 0.08462186)]\n",
      "25 - 17341.txt -> [(5, 0.9178002), (6, 0.076680794)]\n",
      "26 - 17384.txt -> [(5, 0.91687953), (9, 0.077820785)]\n",
      "27 - 17391.txt -> [(9, 0.9863595)]\n",
      "28 - 17398.txt -> [(6, 0.30141962), (8, 0.6924245)]\n",
      "29 - 17399.txt -> [(5, 0.9925598)]\n",
      "30 - 17430.txt -> [(9, 0.9961527)]\n",
      "31 - 17431.txt -> [(8, 0.9959264)]\n",
      "32 - 17447.txt -> [(8, 0.5736545), (9, 0.41886652)]\n",
      "33 - 17457.txt -> [(5, 0.77283007), (6, 0.21426234)]\n",
      "34 - 17460.txt -> [(5, 0.71899503), (8, 0.27359524)]\n",
      "35 - 17501.txt -> [(0, 0.5872869), (4, 0.40001038)]\n",
      "36 - 17518.txt -> [(1, 0.19520728), (2, 0.33290762), (5, 0.27932248), (9, 0.18933573)]\n",
      "37 - 17532.txt -> [(1, 0.074996784), (5, 0.27932236), (6, 0.31088126), (8, 0.05410518), (9, 0.2795203)]\n",
      "38 - 17534.txt -> [(3, 0.6625524), (6, 0.15616223), (9, 0.17621118)]\n",
      "39 - 17578.txt -> [(2, 0.4259313), (5, 0.5664473)]\n",
      "40 - 17609.txt -> [(2, 0.025470518), (5, 0.047920313), (9, 0.9218131)]\n",
      "41 - 17610.txt -> [(5, 0.2723509), (6, 0.7219332)]\n",
      "42 - 17655.txt -> [(2, 0.9945437)]\n",
      "43 - 17662.txt -> [(8, 0.9918157)]\n",
      "44 - 17663.txt -> [(5, 0.15645128), (9, 0.83860874)]\n",
      "45 - 17695.txt -> [(2, 0.3847149), (5, 0.34841418), (9, 0.26056248)]\n",
      "46 - 17711.txt -> [(9, 0.9941541)]\n",
      "47 - 17713.txt -> [(1, 0.98965186)]\n",
      "48 - 17753.txt -> [(3, 0.49152857), (8, 0.5030642)]\n",
      "49 - 17757.txt -> [(5, 0.8540137), (9, 0.14179641)]\n",
      "50 - 17758.txt -> [(4, 0.38610914), (5, 0.131884), (8, 0.47507375)]\n",
      "51 - 17761.txt -> [(4, 0.9927396)]\n",
      "52 - 17803.txt -> [(7, 0.9927979)]\n",
      "53 - 17811.txt -> [(5, 0.5959653), (8, 0.36780828), (9, 0.031077592)]\n",
      "54 - 17874.txt -> [(5, 0.78298527), (9, 0.21410476)]\n",
      "55 - 17879.txt -> [(6, 0.98392403)]\n",
      "56 - 17886.txt -> [(8, 0.9962643)]\n",
      "57 - 17896.txt -> [(3, 0.88266593), (5, 0.02702327), (9, 0.087509945)]\n",
      "58 - 17898.txt -> [(1, 0.99451077)]\n",
      "59 - 17902.txt -> [(1, 0.9945107)]\n",
      "60 - 17912.txt -> [(5, 0.3983635), (9, 0.5979989)]\n",
      "61 - 17933.txt -> [(0, 0.045813832), (5, 0.07885132), (8, 0.21629684), (9, 0.65291363)]\n",
      "62 - 17934.txt -> [(1, 0.88870937), (5, 0.10836527)]\n",
      "63 - 17945.txt -> [(1, 0.99254096)]\n",
      "64 - 17963.txt -> [(5, 0.26120454), (9, 0.7352229)]\n",
      "65 - 17971.txt -> [(1, 0.33422387), (5, 0.65921664)]\n",
      "66 - 17992.txt -> [(8, 0.9927395)]\n",
      "67 - 18004.txt -> [(0, 0.7366908), (1, 0.19465034), (5, 0.06235032)]\n",
      "68 - 18016.txt -> [(1, 0.6771374), (9, 0.31443888)]\n",
      "69 - 18032.txt -> [(0, 0.08431434), (7, 0.58412856), (9, 0.32495105)]\n",
      "70 - 18067.txt -> [(1, 0.44813466), (5, 0.5484577)]\n",
      "71 - 18068.txt -> [(1, 0.0133254165), (6, 0.9830037)]\n",
      "72 - 18080.txt -> [(8, 0.9926807)]\n",
      "73 - 18087.txt -> [(1, 0.994796)]\n",
      "74 - 18088.txt -> [(5, 0.9927397)]\n",
      "75 - 18136.txt -> [(5, 0.9642823), (9, 0.027794287)]\n",
      "76 - 18141.txt -> [(5, 0.045044746), (6, 0.5202136), (8, 0.41867867), (9, 0.013718431)]\n",
      "77 - 18156.txt -> [(5, 0.12563409), (8, 0.8678597)]\n",
      "78 - 18161.txt -> [(5, 0.03275555), (6, 0.4063518), (9, 0.55524564)]\n",
      "79 - 18181.txt -> [(5, 0.068840876), (8, 0.9269693)]\n",
      "80 - 18227.txt -> [(5, 0.9945766)]\n",
      "81 - 18263.txt -> [(0, 0.39539227), (2, 0.375862), (5, 0.19519101), (9, 0.029141467)]\n",
      "82 - 18272.txt -> [(5, 0.018754806), (9, 0.97834575)]\n",
      "83 - 18273.txt -> [(5, 0.96944135), (9, 0.026037391)]\n",
      "84 - 18274.txt -> [(5, 0.14948538), (9, 0.8433695)]\n",
      "85 - 18282.txt -> [(6, 0.99230546)]\n",
      "86 - 18283.txt -> [(5, 0.99188954)]\n",
      "87 - 18307.txt -> [(8, 0.18264091), (9, 0.8117528)]\n",
      "88 - 18368.txt -> [(6, 0.9953354)]\n",
      "89 - 18375.txt -> [(1, 0.63268346), (3, 0.24686813), (9, 0.11466132)]\n",
      "90 - 18376.txt -> [(1, 0.93721294), (6, 0.057764668)]\n",
      "91 - 18396.txt -> [(8, 0.99342835)]\n",
      "92 - 18406.txt -> [(5, 0.17739677), (8, 0.7287716), (9, 0.08804453)]\n",
      "93 - 18413.txt -> [(1, 0.9401981), (5, 0.051799074)]\n",
      "94 - 18414.txt -> [(1, 0.037257187), (5, 0.49523726), (6, 0.36730677), (8, 0.0790374), (9, 0.018303404)]\n",
      "95 - 18447.txt -> [(0, 0.97468024), (5, 0.021885362)]\n",
      "96 - 18473.txt -> [(6, 0.12672341), (8, 0.8688148)]\n",
      "97 - 18480.txt -> [(5, 0.99347645)]\n",
      "98 - 18485.txt -> [(0, 0.027169965), (1, 0.43671903), (3, 0.22408903), (5, 0.14733836), (8, 0.16313516)]\n",
      "99 - 18498.txt -> [(5, 0.993917)]\n",
      "100 - 1858.txt -> [(3, 0.034394655), (5, 0.9594965)]\n",
      "101 - 1859.txt -> [(3, 0.021835335), (5, 0.9715509)]\n",
      "102 - 1860.txt -> [(3, 0.6648962), (5, 0.32926252)]\n",
      "103 - 1864.txt -> [(3, 0.71686655), (8, 0.27831268)]\n",
      "104 - 1865.txt -> [(1, 0.05985226), (3, 0.9341309)]\n",
      "105 - 1866.txt -> [(3, 0.82211727), (5, 0.17199849)]\n",
      "106 - 1867.txt -> [(3, 0.991507)]\n",
      "107 - 1889.txt -> [(3, 0.9921029)]\n",
      "108 - 1891.txt -> [(2, 0.07579815), (3, 0.9134748)]\n",
      "109 - 1908.txt -> [(3, 0.99317956)]\n",
      "110 - 1910.txt -> [(3, 0.51961625), (5, 0.47478768)]\n",
      "111 - 1911.txt -> [(2, 0.34931397), (3, 0.64291656)]\n",
      "112 - 1912.txt -> [(3, 0.99255955)]\n",
      "113 - 1916.txt -> [(2, 0.5894141), (3, 0.40552095)]\n",
      "114 - 1917.txt -> [(2, 0.046176825), (3, 0.8776313), (6, 0.07285738)]\n",
      "115 - 1921.txt -> [(3, 0.9931807)]\n",
      "116 - 1925.txt -> [(2, 0.993615)]\n",
      "117 - 1928.txt -> [(3, 0.82294375), (9, 0.17175661)]\n",
      "118 - 1929.txt -> [(3, 0.25123066), (5, 0.7429281)]\n",
      "119 - 1930.txt -> [(2, 0.2690003), (3, 0.524719), (6, 0.20158133)]\n",
      "120 - 1932.txt -> [(0, 0.77858955), (3, 0.21432875)]\n",
      "121 - 1934.txt -> [(3, 0.9938339)]\n",
      "122 - 1937.txt -> [(3, 0.6579818), (8, 0.33698523)]\n",
      "123 - 1943.txt -> [(1, 0.8156737), (3, 0.1746983)]\n",
      "124 - 1944.txt -> [(1, 0.13426729), (3, 0.85900784)]\n",
      "125 - 1945.txt -> [(2, 0.53264546), (3, 0.46416625)]\n",
      "126 - 1961.txt -> [(3, 0.7345844), (9, 0.26257783)]\n",
      "127 - 1967.txt -> [(3, 0.31285748), (6, 0.6815853)]\n",
      "128 - 1968.txt -> [(3, 0.9924347)]\n",
      "129 - 1974.txt -> [(3, 0.47616643), (5, 0.51883197)]\n",
      "130 - 1975.txt -> [(3, 0.57935965), (5, 0.41604027)]\n",
      "131 - 1976.txt -> [(1, 0.3446298), (3, 0.43864653), (5, 0.07352487), (8, 0.13944764)]\n",
      "132 - 1979.txt -> [(3, 0.8596848), (5, 0.067424186), (8, 0.06912625)]\n",
      "133 - 1981.txt -> [(2, 0.7874532), (3, 0.2094689)]\n",
      "134 - 1984.txt -> [(2, 0.8941843), (3, 0.10044492)]\n",
      "135 - 1985.txt -> [(3, 0.36543387), (5, 0.62943625)]\n",
      "136 - 1990.txt -> [(3, 0.40152448), (5, 0.3085495), (6, 0.28587833)]\n",
      "137 - 1991.txt -> [(0, 0.8235435), (3, 0.1700543)]\n",
      "138 - 1994.txt -> [(2, 0.04237036), (3, 0.17196229), (6, 0.7786652)]\n",
      "139 - 1998.txt -> [(1, 0.23074618), (2, 0.12446313), (3, 0.50956064), (5, 0.13228793)]\n",
      "140 - 2005.txt -> [(2, 0.047460306), (3, 0.9466119)]\n",
      "141 - 2006.txt -> [(3, 0.9938337)]\n",
      "142 - 2007.txt -> [(3, 0.3638889), (5, 0.6322638)]\n",
      "143 - 2008.txt -> [(3, 0.9932815)]\n",
      "144 - 2009.txt -> [(3, 0.75380796), (5, 0.24089235)]\n",
      "145 - 2025.txt -> [(1, 0.35056356), (3, 0.6434645)]\n",
      "146 - 2026.txt -> [(3, 0.99166423)]\n",
      "147 - 2030.txt -> [(3, 0.13879058), (5, 0.1372992), (9, 0.7200416)]\n",
      "148 - 2031.txt -> [(2, 0.81521535), (3, 0.18068084)]\n",
      "149 - 2033.txt -> [(6, 0.9905234)]\n",
      "150 - 2035.txt -> [(1, 0.7522289), (3, 0.24188669)]\n",
      "151 - 2036.txt -> [(3, 0.8110983), (6, 0.14609472), (8, 0.037805323)]\n",
      "152 - 2043.txt -> [(1, 0.2998329), (3, 0.6040822), (6, 0.09225848)]\n",
      "153 - 2045.txt -> [(3, 0.9934285)]\n",
      "154 - 2046.txt -> [(3, 0.9939577)]\n",
      "155 - 2051.txt -> [(2, 0.2500056), (3, 0.2617212), (6, 0.481408)]\n",
      "156 - 2055.txt -> [(5, 0.9961027)]\n",
      "157 - 2058.txt -> [(3, 0.55713946), (8, 0.43730307)]\n",
      "158 - 2059.txt -> [(3, 0.99323076)]\n",
      "159 - 2062.txt -> [(2, 0.9921351)]\n",
      "160 - 2067.txt -> [(0, 0.26757738), (3, 0.5140147), (5, 0.21423984)]\n",
      "161 - 2076.txt -> [(3, 0.5564736), (4, 0.046014488), (6, 0.38464117)]\n",
      "162 - 2079.txt -> [(3, 0.2826623), (6, 0.7121411)]\n",
      "163 - 2080.txt -> [(3, 0.4429195), (5, 0.05528115), (9, 0.49706796)]\n",
      "164 - 2081.txt -> [(3, 0.44665116), (5, 0.5453462)]\n",
      "165 - 2085.txt -> [(3, 0.8251442), (9, 0.16982275)]\n",
      "166 - 2086.txt -> [(1, 0.02840188), (3, 0.578166), (5, 0.38700813)]\n",
      "167 - 2087.txt -> [(5, 0.9854482)]\n",
      "168 - 2089.txt -> [(3, 0.65649515), (5, 0.33570933)]\n",
      "169 - 2090.txt -> [(0, 0.6049857), (3, 0.39013484)]\n",
      "170 - 2091.txt -> [(3, 0.9934286)]\n",
      "171 - 2094.txt -> [(3, 0.81044745), (5, 0.05285197), (9, 0.1325573)]\n",
      "172 - 2095.txt -> [(1, 0.3616326), (3, 0.11347762), (5, 0.5195851)]\n",
      "173 - 2098.txt -> [(3, 0.9938757)]\n",
      "174 - 2099.txt -> [(1, 0.29936525), (2, 0.4106459), (3, 0.25697097), (5, 0.029042982)]\n",
      "175 - 2101.txt -> [(1, 0.051814638), (3, 0.61645055), (4, 0.20755967), (9, 0.11645505)]\n",
      "176 - 2108.txt -> [(0, 0.9471012), (3, 0.046285376)]\n",
      "177 - 2110.txt -> [(3, 0.5011932), (5, 0.47220114), (8, 0.023014795)]\n",
      "178 - 2113.txt -> [(1, 0.25892568), (3, 0.031466905), (5, 0.70471066)]\n",
      "179 - 2115.txt -> [(3, 0.9298446), (5, 0.064992405)]\n"
     ]
    }
   ],
   "source": [
    "for i, corpus_item in zip(range(len(bow_corpus)), bow_corpus):\n",
    "    print(data['RowNum'][i], '-', data['FileName'][i], '->', lda_model[corpus_item])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
