{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relvant packages for conduct topic modeling analysis\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "np.random.seed(2023)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import*\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16748.txt',\n",
       " '17108.txt',\n",
       " '17109.txt',\n",
       " '17110.txt',\n",
       " '17111.txt',\n",
       " '17116.txt',\n",
       " '17117.txt',\n",
       " '17118.txt',\n",
       " '17119.txt',\n",
       " '17139.txt',\n",
       " '17144.txt',\n",
       " '17145.txt',\n",
       " '17146.txt',\n",
       " '17147.txt',\n",
       " '17150.txt',\n",
       " '17185.txt',\n",
       " '17192.txt',\n",
       " '17219.txt',\n",
       " '17239.txt',\n",
       " '17243.txt',\n",
       " '17254.txt',\n",
       " '17255.txt',\n",
       " '17280.txt',\n",
       " '17300.txt',\n",
       " '17303.txt',\n",
       " '17341.txt',\n",
       " '17384.txt',\n",
       " '17391.txt',\n",
       " '17398.txt',\n",
       " '17399.txt',\n",
       " '17430.txt',\n",
       " '17431.txt',\n",
       " '17447.txt',\n",
       " '17457.txt',\n",
       " '17460.txt',\n",
       " '17501.txt',\n",
       " '17518.txt',\n",
       " '17532.txt',\n",
       " '17534.txt',\n",
       " '17578.txt',\n",
       " '17609.txt',\n",
       " '17610.txt',\n",
       " '17655.txt',\n",
       " '17662.txt',\n",
       " '17663.txt',\n",
       " '17695.txt',\n",
       " '17711.txt',\n",
       " '17713.txt',\n",
       " '17753.txt',\n",
       " '17757.txt',\n",
       " '17758.txt',\n",
       " '17761.txt',\n",
       " '17803.txt',\n",
       " '17811.txt',\n",
       " '17874.txt',\n",
       " '17879.txt',\n",
       " '17886.txt',\n",
       " '17896.txt',\n",
       " '17898.txt',\n",
       " '17902.txt',\n",
       " '17912.txt',\n",
       " '17933.txt',\n",
       " '17934.txt',\n",
       " '17945.txt',\n",
       " '17963.txt',\n",
       " '17971.txt',\n",
       " '17992.txt',\n",
       " '18004.txt',\n",
       " '18016.txt',\n",
       " '18032.txt',\n",
       " '18067.txt',\n",
       " '18068.txt',\n",
       " '18080.txt',\n",
       " '18087.txt',\n",
       " '18088.txt',\n",
       " '18136.txt',\n",
       " '18141.txt',\n",
       " '18156.txt',\n",
       " '18161.txt',\n",
       " '18181.txt',\n",
       " '18227.txt',\n",
       " '18263.txt',\n",
       " '18272.txt',\n",
       " '18273.txt',\n",
       " '18274.txt',\n",
       " '18282.txt',\n",
       " '18283.txt',\n",
       " '18307.txt',\n",
       " '18368.txt',\n",
       " '18375.txt',\n",
       " '18376.txt',\n",
       " '18396.txt',\n",
       " '18406.txt',\n",
       " '18413.txt',\n",
       " '18414.txt',\n",
       " '18447.txt',\n",
       " '18473.txt',\n",
       " '18480.txt',\n",
       " '18485.txt',\n",
       " '18498.txt',\n",
       " '1858.txt',\n",
       " '1859.txt',\n",
       " '1860.txt',\n",
       " '1864.txt',\n",
       " '1865.txt',\n",
       " '1866.txt',\n",
       " '1867.txt',\n",
       " '1889.txt',\n",
       " '1891.txt',\n",
       " '1908.txt',\n",
       " '1910.txt',\n",
       " '1911.txt',\n",
       " '1912.txt',\n",
       " '1916.txt',\n",
       " '1917.txt',\n",
       " '1921.txt',\n",
       " '1925.txt',\n",
       " '1928.txt',\n",
       " '1929.txt',\n",
       " '1930.txt',\n",
       " '1932.txt',\n",
       " '1934.txt',\n",
       " '1937.txt',\n",
       " '1943.txt',\n",
       " '1944.txt',\n",
       " '1945.txt',\n",
       " '1961.txt',\n",
       " '1967.txt',\n",
       " '1968.txt',\n",
       " '1974.txt',\n",
       " '1975.txt',\n",
       " '1976.txt',\n",
       " '1979.txt',\n",
       " '1981.txt',\n",
       " '1984.txt',\n",
       " '1985.txt',\n",
       " '1990.txt',\n",
       " '1991.txt',\n",
       " '1994.txt',\n",
       " '1998.txt',\n",
       " '2005.txt',\n",
       " '2006.txt',\n",
       " '2007.txt',\n",
       " '2008.txt',\n",
       " '2009.txt',\n",
       " '2025.txt',\n",
       " '2026.txt',\n",
       " '2030.txt',\n",
       " '2031.txt',\n",
       " '2033.txt',\n",
       " '2035.txt',\n",
       " '2036.txt',\n",
       " '2043.txt',\n",
       " '2045.txt',\n",
       " '2046.txt',\n",
       " '2051.txt',\n",
       " '2055.txt',\n",
       " '2058.txt',\n",
       " '2059.txt',\n",
       " '2062.txt',\n",
       " '2067.txt',\n",
       " '2076.txt',\n",
       " '2079.txt',\n",
       " '2080.txt',\n",
       " '2081.txt',\n",
       " '2085.txt',\n",
       " '2086.txt',\n",
       " '2087.txt',\n",
       " '2089.txt',\n",
       " '2090.txt',\n",
       " '2091.txt',\n",
       " '2094.txt',\n",
       " '2095.txt',\n",
       " '2098.txt',\n",
       " '2099.txt',\n",
       " '2101.txt',\n",
       " '2108.txt',\n",
       " '2110.txt',\n",
       " '2113.txt',\n",
       " '2115.txt']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'MovieReviews' # Folder Name\n",
    "file_names = PlaintextCorpusReader(corpus_root, '.*',encoding='latin-1')  # wildcard is read all files in the folder\n",
    "file_names.fileids()  # Get the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path: str = '/Users/jakebrulato/Documents/GitHub/KDD/Homework_8/MovieReviews'\n",
    "file_names = os.listdir(path)\n",
    "\n",
    "doc_contents: list = []\n",
    "for i, file_name in zip(range(len(file_names)), file_names):\n",
    "    with open(path + '/' + file_name, encoding=\"utf8\", errors='ignore') as file:\n",
    "        doc_contents.append((i, file_name, file.read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNum   FileName                                        FileContent\n",
      "0       0   1912.txt  Date Released: 5/14/93 Running Length: 1:51 Ra...\n",
      "1       1   1866.txt  Joon (Mary Stuart Masterson) is a young woman ...\n",
      "2       2   1867.txt  Starring: Lou Diamond Phillips, Jennifer Tilly...\n",
      "3       3  18087.txt  LOVE IS THE DEVIL (director: John Maybury; Der...\n",
      "4       4   1911.txt  Starring: Thomas Ian Griffith, Lance Henriksen...\n"
     ]
    }
   ],
   "source": [
    "data: DataFrame = pd.DataFrame(doc_contents, columns=['RowNum', 'FileName', 'FileContent'])\n",
    "data.dropna(subset=['FileContent'], inplace= True)\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [date, releas, run, length, rat, violenc, lang...\n",
      "1    [joon, mari, stuart, masterson, young, woman, ...\n",
      "2    [star, diamond, phillip, jennif, tilli, toshir...\n",
      "3    [love, devil, director, john, mayburi, derek, ...\n",
      "4    [star, thoma, griffith, lanc, henriksen, jam, ...\n",
      "Name: FileContent, dtype: object\n"
     ]
    }
   ],
   "source": [
    "processed_docs = data['FileContent'].map(preprocess)\n",
    "print(processed_docs[:5])\n",
    "\n",
    "dictionary: Dictionary = Dictionary(processed_docs)\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "print(len(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"action\") appears 1 times.\n",
      "Word 1 (\"american\") appears 3 times.\n",
      "Word 2 (\"arriv\") appears 1 times.\n",
      "Word 3 (\"attempt\") appears 2 times.\n",
      "Word 4 (\"awar\") appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "test_corpus = bow_corpus[0]\n",
    "for i, word_stat in zip(range(len(test_corpus)), test_corpus):\n",
    "    # print only the first 5 words\n",
    "    if i < 5:\n",
    "        print(\"Word {} (\\\"{}\\\") appears {} times.\".format(word_stat[0], dictionary[word_stat[0]], word_stat[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
