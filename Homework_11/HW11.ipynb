{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love deep learning and text mining'\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'and': 5, 'text': 6, 'mining': 7}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'i', 2: 'love', 3: 'deep', 4: 'learning', 5: 'and', 6: 'text', 7: 'mining'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: wget: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the URL and file path\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "file_path = 'glove.6B.zip'\n",
    "\n",
    "# Download the file\n",
    "os.system(f'wget {url} -O {file_path}')\n",
    "\n",
    "# Unzip the file\n",
    "os.system(f'unzip {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "with open('glove.6B.50d.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word = line.split()[0]\n",
    "        word_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.04662981 -0.00095054 -0.02192831  0.03346461 -0.02455703 -0.01699365\n",
      "   0.0155719   0.0203704   0.00427513 -0.01294947 -0.03519137 -0.00675808\n",
      "  -0.02349998 -0.02286354  0.00773432 -0.02575028 -0.00721722 -0.02094314\n",
      "  -0.02748066 -0.00369291  0.00612719 -0.00023375  0.00434867 -0.00063705\n",
      "  -0.03567322  0.02960384 -0.02691988  0.04315214  0.0164523   0.04246105\n",
      "  -0.03203094 -0.02268632  0.03460028  0.01560796  0.00170945  0.03339574\n",
      "   0.01846895  0.01906809 -0.0081032  -0.01708674  0.02899529 -0.02841201\n",
      "  -0.04682405  0.00248916 -0.01212237  0.04458698 -0.00694709 -0.04113496\n",
      "   0.04215902  0.02787727]], shape=(1, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50  # dimension of the word embeddings\n",
    "vocab_size = len(word_list)  # assuming word_list is defined\n",
    "word_index = {word: index for index, word in enumerate(word_list)}  # assuming word_list is defined\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Get the word index for \"deep\"\n",
    "word = 'deep'\n",
    "word_idx = word_index[word]\n",
    "\n",
    "# Embed the word\n",
    "embedded_word = embedding_layer(tf.constant([word_idx]))\n",
    "\n",
    "# Print the embedded word\n",
    "print(embedded_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.04789047 -0.03611096  0.03067328 -0.01189851 -0.02462651 -0.0353537\n",
      "  -0.0336131   0.00337757 -0.02382137  0.01429733  0.02520097  0.00332152\n",
      "   0.02893979  0.01474389  0.04468125  0.02049011  0.03016585  0.00103295\n",
      "  -0.0031607  -0.03784917  0.02448657  0.02969531  0.02932451  0.00596938\n",
      "  -0.0277768   0.03140045  0.0169558   0.03025651  0.00489876 -0.0096597\n",
      "   0.00305755  0.04376546  0.00863464  0.02891247  0.03754568  0.02411992\n",
      "  -0.03873118 -0.01689482  0.01721006  0.02083113  0.01975257 -0.04101632\n",
      "   0.04578165  0.03468609 -0.04980191  0.02111502  0.02842546  0.01714117\n",
      "  -0.04550761  0.04593566]], shape=(1, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50  # dimension of the word embeddings\n",
    "vocab_size = len(word_list)  # assuming word_list is defined\n",
    "word_index = {word: index for index, word in enumerate(word_list)}  # assuming word_list is defined\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Get the word index for \"deep\"\n",
    "word = 'text'\n",
    "word_idx = word_index[word]\n",
    "\n",
    "# Embed the word\n",
    "embedded_word = embedding_layer(tf.constant([word_idx]))\n",
    "\n",
    "# Print the embedded word\n",
    "print(embedded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'deep': [0.31445, 1.2024, 0.066651, -0.20096, -0.049636, 0.66882, -0.049386, 0.44174, 0.1799, -0.10196, -0.43674, 0.12076, -0.12495, 0.43378, -0.87784, 0.010281, 0.54592, -0.28928, -0.46115, -0.32058, -0.69094, 0.49733, 0.40657, -0.90062, 0.69699, -1.1536, -0.12229, 1.0657, 0.93207, 0.20439, 3.3004, 0.14223, 0.46493, 0.075359, -0.56755, 0.30769, -1.1251, -0.37871, 0.57479, -0.12629, 0.13589, 0.10633, 0.058432, 0.40321, 0.10243, 0.12004, 0.41383, 0.051987, -0.5835, -1.1159]\n",
      "# of Dimensions for 'deep': 50\n",
      "Embedding for 'text': [0.32615, 0.36686, -0.0074905, -0.37553, 0.66715, 0.21646, -0.19801, -1.1001, -0.42221, 0.10574, -0.31292, 0.50953, 0.55775, 0.12019, 0.31441, -0.25043, -1.0637, -1.3213, 0.87798, -0.24627, 0.27379, -0.51092, 0.49324, 0.52243, 1.1636, -0.75323, -0.48053, -0.11259, -0.54595, -0.83921, 2.9825, -1.1916, -0.51958, -0.39365, -0.1419, -0.026977, 0.66296, 0.16574, -1.1681, 0.14443, 1.6305, -0.17216, -0.17436, -0.01049, -0.17794, 0.93076, 1.0381, 0.94266, -0.14805, -0.61109]\n",
      "# of Dimensions for 'text': 50\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file into a dictionary.\n",
    "\n",
    "    Args:\n",
    "    - file_path: Path to the GloVe embeddings file.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where keys are words and values are embeddings.\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = [float(val) for val in values[1:]]\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "def get_embeddings_for_sentence(sentence, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Get embeddings for each word in a sentence.\n",
    "\n",
    "    Args:\n",
    "    - sentence: A string containing multiple words.\n",
    "    - embeddings_dict: A dictionary of word embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary of embeddings for the sentence's words.\n",
    "    \"\"\"\n",
    "    sentence_embeddings = {}\n",
    "    for word in sentence.split():\n",
    "        # Here, we convert words to lowercase to match the GloVe's casing\n",
    "        word_embedding = embeddings_dict.get(word.lower())\n",
    "        if word_embedding is not None:\n",
    "            sentence_embeddings[word] = word_embedding\n",
    "    return sentence_embeddings\n",
    "\n",
    "# Path to the GloVe embeddings file (adjust as necessary)\n",
    "glove_file_path = 'glove.6B.50d.txt'\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "embeddings_dict = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Sentence for which to get embeddings\n",
    "sentence = \"I love deep learning and text mining\"\n",
    "\n",
    "# Get embeddings for the sentence\n",
    "sentence_embeddings = get_embeddings_for_sentence(sentence, embeddings_dict)\n",
    "\n",
    "# For demonstration, let's print the embeddings for \"deep\" and \"text\"\n",
    "print(\"Embedding for 'deep':\", sentence_embeddings.get(\"deep\"))\n",
    "print(\"# of Dimensions for 'deep':\", len(sentence_embeddings.get(\"deep\")))\n",
    "print(\"Embedding for 'text':\", sentence_embeddings.get(\"text\"))\n",
    "print(\"# of Dimensions for 'text':\", len(sentence_embeddings.get(\"text\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
