{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 11 - Word Embedding**\n",
    "# KDD Tuesdays 12:30 PM - 2:45 PM\n",
    "## Jake Brulato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-09 11:18:31.119797: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The Sentence we will be embedding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love deep learning and text mining'\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Vocab Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'and': 5, 'text': 6, 'mining': 7}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Inverse Vocab Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'i', 2: 'love', 3: 'deep', 4: 'learning', 5: 'and', 6: 'text', 7: 'mining'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Sequence of the sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Download Glove**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: wget: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the URL and file path\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "file_path = 'glove.6B.zip'\n",
    "\n",
    "# Download the file\n",
    "os.system(f'wget {url} -O {file_path}')\n",
    "\n",
    "# Unzip the file\n",
    "os.system(f'unzip {file_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Create Word list from Glove.6B.50D**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "with open('glove.6B.50d.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word = line.split()[0]\n",
    "        word_list.append(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Define Dimentions and embedding for the word with Tensor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "with open('glove.6B.50d.txt', encoding = 'utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Embeddings for the word text and deep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.31445 ,  1.2024  ,  0.066651, -0.20096 , -0.049636,  0.66882 ,\n",
       "       -0.049386,  0.44174 ,  0.1799  , -0.10196 , -0.43674 ,  0.12076 ,\n",
       "       -0.12495 ,  0.43378 , -0.87784 ,  0.010281,  0.54592 , -0.28928 ,\n",
       "       -0.46115 , -0.32058 , -0.69094 ,  0.49733 ,  0.40657 , -0.90062 ,\n",
       "        0.69699 , -1.1536  , -0.12229 ,  1.0657  ,  0.93207 ,  0.20439 ,\n",
       "        3.3004  ,  0.14223 ,  0.46493 ,  0.075359, -0.56755 ,  0.30769 ,\n",
       "       -1.1251  , -0.37871 ,  0.57479 , -0.12629 ,  0.13589 ,  0.10633 ,\n",
       "        0.058432,  0.40321 ,  0.10243 ,  0.12004 ,  0.41383 ,  0.051987,\n",
       "       -0.5835  , -1.1159  ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index['deep']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.32615  ,  0.36686  , -0.0074905, -0.37553  ,  0.66715  ,\n",
       "        0.21646  , -0.19801  , -1.1001   , -0.42221  ,  0.10574  ,\n",
       "       -0.31292  ,  0.50953  ,  0.55775  ,  0.12019  ,  0.31441  ,\n",
       "       -0.25043  , -1.0637   , -1.3213   ,  0.87798  , -0.24627  ,\n",
       "        0.27379  , -0.51092  ,  0.49324  ,  0.52243  ,  1.1636   ,\n",
       "       -0.75323  , -0.48053  , -0.11259  , -0.54595  , -0.83921  ,\n",
       "        2.9825   , -1.1916   , -0.51958  , -0.39365  , -0.1419   ,\n",
       "       -0.026977 ,  0.66296  ,  0.16574  , -1.1681   ,  0.14443  ,\n",
       "        1.6305   , -0.17216  , -0.17436  , -0.01049  , -0.17794  ,\n",
       "        0.93076  ,  1.0381   ,  0.94266  , -0.14805  , -0.61109  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_index['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Embedding Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size+1, 50))\n",
    "for word, i in inverse_vocab.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Consolidated Embeddings for the word text and deep**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for 'deep': [0.31445, 1.2024, 0.066651, -0.20096, -0.049636, 0.66882, -0.049386, 0.44174, 0.1799, -0.10196, -0.43674, 0.12076, -0.12495, 0.43378, -0.87784, 0.010281, 0.54592, -0.28928, -0.46115, -0.32058, -0.69094, 0.49733, 0.40657, -0.90062, 0.69699, -1.1536, -0.12229, 1.0657, 0.93207, 0.20439, 3.3004, 0.14223, 0.46493, 0.075359, -0.56755, 0.30769, -1.1251, -0.37871, 0.57479, -0.12629, 0.13589, 0.10633, 0.058432, 0.40321, 0.10243, 0.12004, 0.41383, 0.051987, -0.5835, -1.1159]\n",
      "# of Dimensions for 'deep': 50\n",
      "Embedding for 'text': [0.32615, 0.36686, -0.0074905, -0.37553, 0.66715, 0.21646, -0.19801, -1.1001, -0.42221, 0.10574, -0.31292, 0.50953, 0.55775, 0.12019, 0.31441, -0.25043, -1.0637, -1.3213, 0.87798, -0.24627, 0.27379, -0.51092, 0.49324, 0.52243, 1.1636, -0.75323, -0.48053, -0.11259, -0.54595, -0.83921, 2.9825, -1.1916, -0.51958, -0.39365, -0.1419, -0.026977, 0.66296, 0.16574, -1.1681, 0.14443, 1.6305, -0.17216, -0.17436, -0.01049, -0.17794, 0.93076, 1.0381, 0.94266, -0.14805, -0.61109]\n",
      "# of Dimensions for 'text': 50\n"
     ]
    }
   ],
   "source": [
    "def load_glove_embeddings(file_path):\n",
    "    \"\"\"\n",
    "    Load GloVe embeddings from a file into a dictionary.\n",
    "\n",
    "    Args:\n",
    "    - file_path: Path to the GloVe embeddings file.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary where keys are words and values are embeddings.\n",
    "    \"\"\"\n",
    "    embeddings_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = [float(val) for val in values[1:]]\n",
    "            embeddings_dict[word] = vector\n",
    "    return embeddings_dict\n",
    "\n",
    "def get_embeddings_for_sentence(sentence, embeddings_dict):\n",
    "    \"\"\"\n",
    "    Get embeddings for each word in a sentence.\n",
    "\n",
    "    Args:\n",
    "    - sentence: A string containing multiple words.\n",
    "    - embeddings_dict: A dictionary of word embeddings.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary of embeddings for the sentence's words.\n",
    "    \"\"\"\n",
    "    sentence_embeddings = {}\n",
    "    for word in sentence.split():\n",
    "        # Here, we convert words to lowercase to match the GloVe's casing\n",
    "        word_embedding = embeddings_dict.get(word.lower())\n",
    "        if word_embedding is not None:\n",
    "            sentence_embeddings[word] = word_embedding\n",
    "    return sentence_embeddings\n",
    "\n",
    "# Path to the GloVe embeddings file (adjust as necessary)\n",
    "glove_file_path = 'glove.6B.50d.txt'\n",
    "\n",
    "# Load the GloVe embeddings\n",
    "embeddings_dict = load_glove_embeddings(glove_file_path)\n",
    "\n",
    "# Sentence for which to get embeddings\n",
    "sentence = \"I love deep learning and text mining\"\n",
    "\n",
    "# Get embeddings for the sentence\n",
    "sentence_embeddings = get_embeddings_for_sentence(sentence, embeddings_dict)\n",
    "\n",
    "# For demonstration, let's print the embeddings for \"deep\" and \"text\"\n",
    "print(\"Embedding for 'deep':\", sentence_embeddings.get(\"deep\"))\n",
    "print(\"# of Dimensions for 'deep':\", len(sentence_embeddings.get(\"deep\")))\n",
    "print(\"Embedding for 'text':\", sentence_embeddings.get(\"text\"))\n",
    "print(\"# of Dimensions for 'text':\", len(sentence_embeddings.get(\"text\")))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
