{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 12:07:09.493055: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I love deep learning and text mining'\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<pad>': 0, 'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'and': 5, 'text': 6, 'mining': 7}\n"
     ]
    }
   ],
   "source": [
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<pad>', 1: 'i', 2: 'love', 3: 'deep', 4: 'learning', 5: 'and', 6: 'text', 7: 'mining'}\n"
     ]
    }
   ],
   "source": [
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: wget: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the URL and file path\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "file_path = 'glove.6B.zip'\n",
    "\n",
    "# Download the file\n",
    "os.system(f'wget {url} -O {file_path}')\n",
    "\n",
    "# Unzip the file\n",
    "os.system(f'unzip {file_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "with open('glove.6B.50d.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        word = line.split()[0]\n",
    "        word_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-5.96530363e-03 -1.77370794e-02 -2.67687794e-02  4.87301983e-02\n",
      "   2.97187641e-03  2.70091034e-02  4.51920182e-03  4.67146561e-03\n",
      "  -1.40277520e-02  2.59719528e-02  3.07676680e-02  4.82952632e-02\n",
      "  -1.88554060e-02 -1.99844372e-02 -2.16402411e-02  1.76133551e-02\n",
      "   4.73482348e-02  4.63599600e-02  1.82350762e-02  6.15967438e-03\n",
      "  -2.46704742e-03  2.95943134e-02 -3.85503657e-02 -4.08244021e-02\n",
      "   3.00166868e-02 -2.54029520e-02  1.17248781e-02  3.29272188e-02\n",
      "  -2.62972955e-02  1.10916495e-02 -2.57517342e-02  3.86709906e-02\n",
      "   1.99380554e-02 -3.35190073e-02 -3.92777696e-02 -1.10898837e-02\n",
      "  -6.37091696e-04 -3.12340986e-02 -4.89923246e-02  3.27336304e-02\n",
      "   1.36715434e-02 -3.18026915e-02 -5.36553562e-05 -4.23434861e-02\n",
      "  -1.33789405e-02 -8.36379454e-03  4.92415912e-02  3.18757631e-02\n",
      "   1.38673522e-02  3.63260545e-02]], shape=(1, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50  # dimension of the word embeddings\n",
    "vocab_size = len(word_list)  # assuming word_list is defined\n",
    "word_index = {word: index for index, word in enumerate(word_list)}  # assuming word_list is defined\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Get the word index for \"deep\"\n",
    "word = 'deep'\n",
    "word_idx = word_index[word]\n",
    "\n",
    "# Embed the word\n",
    "embedded_word = embedding_layer(tf.constant([word_idx]))\n",
    "\n",
    "# Print the embedded word\n",
    "print(embedded_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.02486375 -0.04828658 -0.03965479 -0.02628337 -0.04926895  0.04554569\n",
      "   0.03163556 -0.02948011 -0.04752111 -0.03913945 -0.03148532 -0.03444215\n",
      "  -0.0415091  -0.00701737  0.04726838  0.02648583 -0.00315119  0.02229295\n",
      "   0.03758762 -0.04191985  0.03904814  0.00602818 -0.02913541  0.02452577\n",
      "  -0.02033213 -0.02860898 -0.04983724 -0.03539735  0.04373798  0.00303889\n",
      "   0.02709284 -0.00589546  0.04058797 -0.03784945  0.03285468 -0.01706291\n",
      "  -0.01228501  0.03564537 -0.00445639  0.02145975  0.04460683  0.00502164\n",
      "  -0.04017957  0.02416552  0.03413532  0.02631617 -0.04463227  0.04534226\n",
      "   0.04129114 -0.03513554]], shape=(1, 50), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 50  # dimension of the word embeddings\n",
    "vocab_size = len(word_list)  # assuming word_list is defined\n",
    "word_index = {word: index for index, word in enumerate(word_list)}  # assuming word_list is defined\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer = layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Get the word index for \"deep\"\n",
    "word = 'text'\n",
    "word_idx = word_index[word]\n",
    "\n",
    "# Embed the word\n",
    "embedded_word = embedding_layer(tf.constant([word_idx]))\n",
    "\n",
    "# Print the embedded word\n",
    "print(embedded_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
