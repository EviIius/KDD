{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy==1.10.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: numpy==1.23.5 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (1.23.5)\n",
      "Collecting git+https://github.com/stephenhky/PyShortTextCategorization\n",
      "  Cloning https://github.com/stephenhky/PyShortTextCategorization to /private/var/folders/6s/bfbnxg_x46gfcltpcwwr67200000gn/T/pip-req-build-y3s50em6\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/stephenhky/PyShortTextCategorization /private/var/folders/6s/bfbnxg_x46gfcltpcwwr67200000gn/T/pip-req-build-y3s50em6\n",
      "  Resolved https://github.com/stephenhky/PyShortTextCategorization to commit e183bac2a362051087e3cb9160ccd8f2ee67f315\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting Cython>=3.0.0 (from shorttext==1.6.1)\n",
      "  Using cached Cython-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numpy>=1.23.3 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from shorttext==1.6.1) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from shorttext==1.6.1) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.3.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from shorttext==1.6.1) (1.3.2)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from shorttext==1.6.1) (1.4.0)\n",
      "Collecting tensorflow>=2.13.0 (from shorttext==1.6.1)\n",
      "  Downloading tensorflow-2.15.0-cp310-cp310-macosx_10_15_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting keras>=2.13.0 (from shorttext==1.6.1)\n",
      "  Downloading keras-3.0.5-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting gensim>=4.0.0 (from shorttext==1.6.1)\n",
      "  Downloading gensim-4.3.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from shorttext==1.6.1) (2.2.0)\n",
      "Collecting snowballstemmer>=2.0.0 (from shorttext==1.6.1)\n",
      "  Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting transformers>=4.32.0 (from shorttext==1.6.1)\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch>=2.0.0 (from shorttext==1.6.1)\n",
      "  Downloading torch-2.2.1-cp310-none-macosx_10_9_x86_64.whl.metadata (25 kB)\n",
      "Collecting python-Levenshtein>=0.21.0 (from shorttext==1.6.1)\n",
      "  Downloading python_Levenshtein-0.25.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting numba>=0.57.0 (from shorttext==1.6.1)\n",
      "  Downloading numba-0.59.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting smart-open>=1.8.1 (from gensim>=4.0.0->shorttext==1.6.1)\n",
      "  Downloading smart_open-7.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting absl-py (from keras>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=2.13.0->shorttext==1.6.1)\n",
      "  Using cached rich-13.7.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting h5py (from keras>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading h5py-3.10.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting dm-tree (from keras>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading dm_tree-0.1.8-cp310-cp310-macosx_10_9_x86_64.whl.metadata (1.9 kB)\n",
      "Collecting ml-dtypes (from keras>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting llvmlite<0.43,>=0.42.0dev0 (from numba>=0.57.0->shorttext==1.6.1)\n",
      "  Downloading llvmlite-0.42.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from pandas>=1.2.0->shorttext==1.6.1) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from pandas>=1.2.0->shorttext==1.6.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from pandas>=1.2.0->shorttext==1.6.1) (2024.1)\n",
      "Collecting Levenshtein==0.25.0 (from python-Levenshtein>=0.21.0->shorttext==1.6.1)\n",
      "  Downloading Levenshtein-0.25.0-cp310-cp310-macosx_10_9_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.1.0 (from Levenshtein==0.25.0->python-Levenshtein>=0.21.0->shorttext==1.6.1)\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from scikit-learn>=1.2.0->shorttext==1.6.1) (3.2.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes (from keras>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading ml_dtypes-0.2.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (23.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions>=3.6.6 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting wrapt<1.15,>=1.11.0 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-macosx_10_14_x86_64.whl.metadata (14 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading grpcio-1.62.0-cp310-cp310-macosx_12_0_universal2.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.16,>=2.15 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras>=2.13.0 (from shorttext==1.6.1)\n",
      "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting filelock (from torch>=2.0.0->shorttext==1.6.1)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting sympy (from torch>=2.0.0->shorttext==1.6.1)\n",
      "  Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=2.0.0->shorttext==1.6.1)\n",
      "  Downloading networkx-3.2.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting jinja2 (from torch>=2.0.0->shorttext==1.6.1)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting fsspec (from torch>=2.0.0->shorttext==1.6.1)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Downloading huggingface_hub-0.21.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pyyaml>=5.1 (from transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Using cached PyYAML-6.0.1-cp310-cp310-macosx_10_9_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from transformers>=4.32.0->shorttext==1.6.1) (2023.12.25)\n",
      "Collecting requests (from transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-macosx_10_12_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from transformers>=4.32.0->shorttext==1.6.1) (4.66.2)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Using cached wheel-0.42.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading google_auth-2.28.1-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading Markdown-3.5.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Using cached charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers>=4.32.0->shorttext==1.6.1)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=2.0.0->shorttext==1.6.1)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch>=2.0.0->shorttext==1.6.1)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading cachetools-5.3.3-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=2.13.0->shorttext==1.6.1)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/jakebrulato/Documents/GitHub/KDD/.venv/lib/python3.10/site-packages (from rich->keras>=2.13.0->shorttext==1.6.1) (2.17.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=2.13.0->shorttext==1.6.1)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached Cython-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl (3.1 MB)\n",
      "Downloading gensim-4.3.2-cp310-cp310-macosx_10_9_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.59.0-cp310-cp310-macosx_10_9_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_Levenshtein-0.25.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading Levenshtein-0.25.0-cp310-cp310-macosx_10_9_x86_64.whl (135 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.5/135.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow-2.15.0-cp310-cp310-macosx_10_15_x86_64.whl (239.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading torch-2.2.1-cp310-none-macosx_10_9_x86_64.whl (150.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.8/150.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading grpcio-1.62.0-cp310-cp310-macosx_12_0_universal2.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.10.0-cp310-cp310-macosx_10_9_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.1-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.1/346.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading llvmlite-0.42.0-cp310-cp310-macosx_10_9_x86_64.whl (31.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.1/31.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0mm00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.2.0-cp310-cp310-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyYAML-6.0.1-cp310-cp310-macosx_10_9_x86_64.whl (189 kB)\n",
      "Downloading safetensors-0.4.2-cp310-cp310-macosx_10_12_x86_64.whl (426 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m426.3/426.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading smart_open-7.0.1-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-macosx_10_14_x86_64.whl (2.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading tokenizers-0.15.2-cp310-cp310-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading wrapt-1.14.1-cp310-cp310-macosx_10_9_x86_64.whl (35 kB)\n",
      "Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached charset_normalizer-3.3.2-cp310-cp310-macosx_10_9_x86_64.whl (122 kB)\n",
      "Downloading google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.9/186.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-macosx_10_9_x86_64.whl (14 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.6.1-cp310-cp310-macosx_10_9_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-macosx_10_9_x86_64.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached wheel-0.42.0-py3-none-any.whl (65 kB)\n",
      "Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: shorttext\n",
      "  Building wheel for shorttext (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for shorttext: filename=shorttext-1.6.1-cp310-cp310-macosx_10_9_universal2.whl size=482160 sha256=4f532cbb6e893ec9b599ba68879dd150a03845423d882918bf8fc3e693e541bf\n",
      "  Stored in directory: /private/var/folders/6s/bfbnxg_x46gfcltpcwwr67200000gn/T/pip-ephem-wheel-cache-5xm7ltev/wheels/84/07/3e/bb56d79459e5c3c9111c649d174481179ef9258e9559bdd72c\n",
      "Successfully built shorttext\n",
      "Installing collected packages: snowballstemmer, mpmath, libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, sympy, safetensors, rapidfuzz, pyyaml, pyasn1, protobuf, opt-einsum, oauthlib, networkx, ml-dtypes, MarkupSafe, markdown, llvmlite, keras, idna, h5py, grpcio, google-pasta, gast, fsspec, filelock, Cython, charset-normalizer, certifi, cachetools, absl-py, werkzeug, smart-open, rsa, requests, pyasn1-modules, numba, Levenshtein, jinja2, astunparse, torch, requests-oauthlib, python-Levenshtein, huggingface-hub, google-auth, gensim, tokenizers, google-auth-oauthlib, transformers, tensorboard, tensorflow, shorttext\n",
      "Successfully installed Cython-3.0.8 Levenshtein-0.25.0 MarkupSafe-2.1.5 absl-py-2.1.0 astunparse-1.6.3 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.1 flatbuffers-23.5.26 fsspec-2024.2.0 gast-0.5.4 gensim-4.3.2 google-auth-2.28.1 google-auth-oauthlib-1.2.0 google-pasta-0.2.0 grpcio-1.62.0 h5py-3.10.0 huggingface-hub-0.21.1 idna-3.6 jinja2-3.1.3 keras-2.15.0 libclang-16.0.6 llvmlite-0.42.0 markdown-3.5.2 ml-dtypes-0.2.0 mpmath-1.3.0 networkx-3.2.1 numba-0.59.0 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.25.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 python-Levenshtein-0.25.0 pyyaml-6.0.1 rapidfuzz-3.6.1 requests-2.31.0 requests-oauthlib-1.3.1 rsa-4.9 safetensors-0.4.2 shorttext-1.6.1 smart-open-7.0.1 snowballstemmer-2.2.0 sympy-1.12 tensorboard-2.15.2 tensorboard-data-server-0.7.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 tensorflow-io-gcs-filesystem-0.36.0 termcolor-2.4.0 tokenizers-0.15.2 torch-2.2.1 transformers-4.38.1 typing-extensions-4.10.0 urllib3-2.2.1 werkzeug-3.0.1 wheel-0.42.0 wrapt-1.14.1\n",
      "/Users/jakebrulato/Documents/GitHub/KDD/.venv/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scipy==1.10.0 numpy==1.23.5\n",
    "!pip install -U git+https://github.com/stephenhky/PyShortTextCategorization\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16748.txt',\n",
       " '17108.txt',\n",
       " '17109.txt',\n",
       " '17110.txt',\n",
       " '17111.txt',\n",
       " '17116.txt',\n",
       " '17117.txt',\n",
       " '17118.txt',\n",
       " '17119.txt',\n",
       " '17139.txt',\n",
       " '17144.txt',\n",
       " '17145.txt',\n",
       " '17146.txt',\n",
       " '17147.txt',\n",
       " '17150.txt',\n",
       " '17185.txt',\n",
       " '17192.txt',\n",
       " '17219.txt',\n",
       " '17239.txt',\n",
       " '17243.txt',\n",
       " '17254.txt',\n",
       " '17255.txt',\n",
       " '17280.txt',\n",
       " '17300.txt',\n",
       " '17303.txt',\n",
       " '17341.txt',\n",
       " '17384.txt',\n",
       " '17391.txt',\n",
       " '17398.txt',\n",
       " '17399.txt',\n",
       " '17430.txt',\n",
       " '17431.txt',\n",
       " '17447.txt',\n",
       " '17457.txt',\n",
       " '17460.txt',\n",
       " '17501.txt',\n",
       " '17518.txt',\n",
       " '17532.txt',\n",
       " '17534.txt',\n",
       " '17578.txt',\n",
       " '17609.txt',\n",
       " '17610.txt',\n",
       " '17655.txt',\n",
       " '17662.txt',\n",
       " '17663.txt',\n",
       " '17695.txt',\n",
       " '17711.txt',\n",
       " '17713.txt',\n",
       " '17753.txt',\n",
       " '17757.txt',\n",
       " '17758.txt',\n",
       " '17761.txt',\n",
       " '17803.txt',\n",
       " '17811.txt',\n",
       " '17874.txt',\n",
       " '17879.txt',\n",
       " '17886.txt',\n",
       " '17896.txt',\n",
       " '17898.txt',\n",
       " '17902.txt',\n",
       " '17912.txt',\n",
       " '17933.txt',\n",
       " '17934.txt',\n",
       " '17945.txt',\n",
       " '17963.txt',\n",
       " '17971.txt',\n",
       " '17992.txt',\n",
       " '18004.txt',\n",
       " '18016.txt',\n",
       " '18032.txt',\n",
       " '18067.txt',\n",
       " '18068.txt',\n",
       " '18080.txt',\n",
       " '18087.txt',\n",
       " '18088.txt',\n",
       " '18136.txt',\n",
       " '18141.txt',\n",
       " '18156.txt',\n",
       " '18161.txt',\n",
       " '18181.txt',\n",
       " '18227.txt',\n",
       " '18263.txt',\n",
       " '18272.txt',\n",
       " '18273.txt',\n",
       " '18274.txt',\n",
       " '18282.txt',\n",
       " '18283.txt',\n",
       " '18307.txt',\n",
       " '18368.txt',\n",
       " '18375.txt',\n",
       " '18376.txt',\n",
       " '18396.txt',\n",
       " '18406.txt',\n",
       " '18413.txt',\n",
       " '18414.txt',\n",
       " '18447.txt',\n",
       " '18473.txt',\n",
       " '18480.txt',\n",
       " '18485.txt',\n",
       " '18498.txt',\n",
       " '1858.txt',\n",
       " '1859.txt',\n",
       " '1860.txt',\n",
       " '1864.txt',\n",
       " '1865.txt',\n",
       " '1866.txt',\n",
       " '1867.txt',\n",
       " '1889.txt',\n",
       " '1891.txt',\n",
       " '1908.txt',\n",
       " '1910.txt',\n",
       " '1911.txt',\n",
       " '1912.txt',\n",
       " '1916.txt',\n",
       " '1917.txt',\n",
       " '1921.txt',\n",
       " '1925.txt',\n",
       " '1928.txt',\n",
       " '1929.txt',\n",
       " '1930.txt',\n",
       " '1932.txt',\n",
       " '1934.txt',\n",
       " '1937.txt',\n",
       " '1943.txt',\n",
       " '1944.txt',\n",
       " '1945.txt',\n",
       " '1961.txt',\n",
       " '1967.txt',\n",
       " '1968.txt',\n",
       " '1974.txt',\n",
       " '1975.txt',\n",
       " '1976.txt',\n",
       " '1979.txt',\n",
       " '1981.txt',\n",
       " '1984.txt',\n",
       " '1985.txt',\n",
       " '1990.txt',\n",
       " '1991.txt',\n",
       " '1994.txt',\n",
       " '1998.txt',\n",
       " '2005.txt',\n",
       " '2006.txt',\n",
       " '2007.txt',\n",
       " '2008.txt',\n",
       " '2009.txt',\n",
       " '2025.txt',\n",
       " '2026.txt',\n",
       " '2030.txt',\n",
       " '2031.txt',\n",
       " '2033.txt',\n",
       " '2035.txt',\n",
       " '2036.txt',\n",
       " '2043.txt',\n",
       " '2045.txt',\n",
       " '2046.txt',\n",
       " '2051.txt',\n",
       " '2055.txt',\n",
       " '2058.txt',\n",
       " '2059.txt',\n",
       " '2062.txt',\n",
       " '2067.txt',\n",
       " '2076.txt',\n",
       " '2079.txt',\n",
       " '2080.txt',\n",
       " '2081.txt',\n",
       " '2085.txt',\n",
       " '2086.txt',\n",
       " '2087.txt',\n",
       " '2089.txt',\n",
       " '2090.txt',\n",
       " '2091.txt',\n",
       " '2094.txt',\n",
       " '2095.txt',\n",
       " '2098.txt',\n",
       " '2099.txt',\n",
       " '2101.txt',\n",
       " '2108.txt',\n",
       " '2110.txt',\n",
       " '2113.txt',\n",
       " '2115.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "corpus_root = 'MovieReviews' # Folder Name\n",
    "filelists = PlaintextCorpusReader(corpus_root, '.*',encoding='latin-1')  # wildcard is read all files in the folder\n",
    "filelists.fileids()  # Get the filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of Reviews \n",
    "reviews = []\n",
    "for fileid in filelists.fileids():\n",
    "    reviews.append(filelists.raw(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THE BED YOU SLEEP IN (director: Jon Jost; cast: Ray Weiss (Tom Blair), Doug (Marshall Gaddis), Beth (Kate Sannella), Scott (Brad Shelton), Mrs. Weiss (Ellen McLaughlin), 1993)\\nTruly independent filmmaker, Jost, has completed his so-called trilogy about rural America with this film and has since moved on to self- imposed exile by going to Europe. This extra-ordinary film offers a long hard look at its subject matter, as the camera is held steadfast, not moving for insatiably long periods of time, picking up all the appropriate nuances it needs to with deliberate dispassion, as it looks at an Oregon lumber mill, whose owner (Tom Blair) is faced with unsettling economic news about the business he has built-up and worked at for his 50- odd years of life. It focuses on this man and tries to find out who he is, using him as a metaphoric symbol for America, perhaps, emulating Emerson\\'s views, as his writings are flashed on screen, exhibiting some sayings from his essays on nature and America.\\nBy seeing who this man is, we get to see how he adjusts to his carefully scripted life, the fly-fishing he loves to do for the sport of it, his easy and almost gentile manners, and his very definite American persona, as he is forced out of economic necessity to deal with the Japanese businessmen he inherently despises, and we get a picture of a rather complicated individual, who has difficulty in communicating with himself and others, so the closer we get to him, the more we sense that there are a lot of things about him that remain unanswered. The shocker about his life that is about to unfold, comes after he meets a foreign stranger on the street, raving about the day of atonement coming soon and how God knows all, that he should pray with him, but is told by him that he has no time for that, as he feels uncomfortable being around this religious zealot, so he fumbles around with his wad of bills and thrusts a few dollars in the preacher\\'s pockets, that are not kindly received by the preacher, as he quickly departs from the preacher\\'s shouts that he doesn\\'t want his money.\\nOur perceptions of him, as a Rock of Gibraltor type, is squelched for good, as we see him come unglued in his very comfortable home, as he interacts with his wife (Ellen McLaughlin), his second wife, as she confronts him with a letter from her college-aged daughter, Tracy, who is his daughter via his first marriage and therefore not Ellen\\'s real daughter. Ellen insists on reading a letter addressed to her from Tracy, out loud, accusing him of placing his hands on her private places, as he responds to his wife\\'s question, all she wants to know, is it true? And all he can respond, is that he wonders why Tracy is doing this to him, saying that she is probably mixed up. What results is apocalyptic, as the film becomes disturbingly mysterious and evasive, never settling for sure who is telling the truth, but destroying the family as it is. This scene could also be deemed as an attack on America\\'s soul, exposing it to questions about truth and character, principles that are put under the microscope, as the story builds to its very tragic outcome.\\nThis is one of Jost\\'s deepest and most penetrating films, it could even be argued that he has made a classical film, as it forcefully and subtly tells an American story, replete with unanswered questions about family life that are haunting, that give the film a certain power that makes you think for a long time afterwards what is it about this country that is so raw and violent in nature, that becomes a part of the people\\'s own nature.\\nOne of the scenes that I found most memorable, was when the camera panned the diner where Tom was dining with some co-workers and all we could hear, at first, was the muffled conversations of the patrons, as the camera meticulously panned the diner, until the atmosphere of the place was fully absorbed and we returned to Tom and his conversation, which became clearer, as this scene played out a daily experience most Americans have had but has rarely been captured so exactly on film. This time consuming shot, is not attempted by commercial filmmakers who live in fear of losing their audience in a long non-action shot.This is one of Jost\\'s strong points, his willingness to explore territory others fear to go.\\nJost\\'s film can be justifiably criticized for a few lapses in the story line it didn\\'t clarify more precisely, but more importantly, it should be praised for the poetry it brings to its story when telling about a malaise in the American culture that is difficult to come to grips with, as the American landscape is perceived as so beautiful a sight to behold and the country as so wealthy a place when compared with the rest of the world, as it asks... But, what does this mean if Americans are not a happy people?\\nREVIEWED ON 3/20/99\\nDennis Schwartz: \"Movie Reviews\"\\n=A9 ALL RIGHTS RESERVED DENNIS SCHWARTZ\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the shorttext library for text preprocessing\n",
    "#standard_text_preprocessor_1 under shorttext.utils provides a standar way of text preprocessing, including the following steps:\n",
    "\n",
    "   #1. removing special characters,\n",
    "   #2. removing numerals,\n",
    "   #3. converting all alphabets to lower cases,\n",
    "   #4. removing stop words, and\n",
    "   #5. stemming the words (using Porter stemmer).\n",
    "\n",
    "from shorttext.utils import standard_text_preprocessor_1, DocumentTermMatrix\n",
    "preprocessor = standard_text_preprocessor_1()\n",
    "corpus = [preprocessor(article).split(' ') for article in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bed',\n",
       " 'sleep',\n",
       " 'director',\n",
       " 'jon',\n",
       " 'jost',\n",
       " 'cast',\n",
       " 'rai',\n",
       " 'weiss',\n",
       " 'tom',\n",
       " 'blair',\n",
       " 'doug',\n",
       " 'marshal',\n",
       " 'gaddi',\n",
       " 'beth',\n",
       " 'kate',\n",
       " 'sannella',\n",
       " 'scott',\n",
       " 'brad',\n",
       " 'shelton',\n",
       " 'mr',\n",
       " 'weiss',\n",
       " 'mclaughlin',\n",
       " '\\ntruli',\n",
       " 'independ',\n",
       " 'filmmak',\n",
       " 'jost',\n",
       " 'complet',\n",
       " 'socal',\n",
       " 'trilogi',\n",
       " 'rural',\n",
       " 'america',\n",
       " 'film',\n",
       " 'sinc',\n",
       " 'move',\n",
       " 'self',\n",
       " 'impos',\n",
       " 'exil',\n",
       " 'go',\n",
       " 'europ',\n",
       " 'extraordinari',\n",
       " 'film',\n",
       " 'offer',\n",
       " 'long',\n",
       " 'hard',\n",
       " 'look',\n",
       " 'subject',\n",
       " 'matter',\n",
       " 'camera',\n",
       " 'held',\n",
       " 'steadfast',\n",
       " 'move',\n",
       " 'insati',\n",
       " 'long',\n",
       " 'period',\n",
       " 'time',\n",
       " 'pick',\n",
       " 'appropri',\n",
       " 'nuanc',\n",
       " 'need',\n",
       " 'deliber',\n",
       " 'dispass',\n",
       " 'look',\n",
       " 'oregon',\n",
       " 'lumber',\n",
       " 'mill',\n",
       " 'whose',\n",
       " 'owner',\n",
       " 'tom',\n",
       " 'blair',\n",
       " 'face',\n",
       " 'unsettl',\n",
       " 'econom',\n",
       " 'new',\n",
       " 'busi',\n",
       " 'builtup',\n",
       " 'work',\n",
       " '',\n",
       " 'odd',\n",
       " 'year',\n",
       " 'life',\n",
       " 'focus',\n",
       " 'tri',\n",
       " 'find',\n",
       " 'us',\n",
       " 'metaphor',\n",
       " 'symbol',\n",
       " 'america',\n",
       " 'perhap',\n",
       " 'emul',\n",
       " 'emerson',\n",
       " 'view',\n",
       " 'write',\n",
       " 'flash',\n",
       " 'screen',\n",
       " 'exhibit',\n",
       " 'sai',\n",
       " 'essai',\n",
       " 'natur',\n",
       " 'america\\nbi',\n",
       " 'see',\n",
       " 'get',\n",
       " 'see',\n",
       " 'adjust',\n",
       " 'carefulli',\n",
       " 'script',\n",
       " 'life',\n",
       " 'flyfish',\n",
       " 'love',\n",
       " 'sport',\n",
       " 'easi',\n",
       " 'almost',\n",
       " 'gentil',\n",
       " 'manner',\n",
       " 'definit',\n",
       " 'american',\n",
       " 'persona',\n",
       " 'forc',\n",
       " 'econom',\n",
       " 'necess',\n",
       " 'deal',\n",
       " 'japanes',\n",
       " 'businessmen',\n",
       " 'inher',\n",
       " 'despis',\n",
       " 'get',\n",
       " 'pictur',\n",
       " 'rather',\n",
       " 'complic',\n",
       " 'individu',\n",
       " 'difficulti',\n",
       " 'commun',\n",
       " 'other',\n",
       " 'closer',\n",
       " 'get',\n",
       " 'sens',\n",
       " 'lot',\n",
       " 'thing',\n",
       " 'remain',\n",
       " 'unansw',\n",
       " 'shocker',\n",
       " 'life',\n",
       " 'unfold',\n",
       " 'come',\n",
       " 'meet',\n",
       " 'foreign',\n",
       " 'stranger',\n",
       " 'street',\n",
       " 'rave',\n",
       " 'dai',\n",
       " 'aton',\n",
       " 'come',\n",
       " 'soon',\n",
       " 'god',\n",
       " 'know',\n",
       " 'prai',\n",
       " 'told',\n",
       " 'time',\n",
       " 'feel',\n",
       " 'uncomfort',\n",
       " 'around',\n",
       " 'religi',\n",
       " 'zealot',\n",
       " 'fumbl',\n",
       " 'around',\n",
       " 'wad',\n",
       " 'bill',\n",
       " 'thrust',\n",
       " 'dollar',\n",
       " 'preacher',\n",
       " 'pocket',\n",
       " 'kindli',\n",
       " 'receiv',\n",
       " 'preacher',\n",
       " 'quickli',\n",
       " 'depart',\n",
       " 'preacher',\n",
       " 'shout',\n",
       " 'doesnt',\n",
       " 'money\\nour',\n",
       " 'percept',\n",
       " 'rock',\n",
       " 'gibraltor',\n",
       " 'type',\n",
       " 'squelch',\n",
       " 'good',\n",
       " 'see',\n",
       " 'unglu',\n",
       " 'comfort',\n",
       " 'home',\n",
       " 'interact',\n",
       " 'wife',\n",
       " 'mclaughlin',\n",
       " 'second',\n",
       " 'wife',\n",
       " 'confront',\n",
       " 'letter',\n",
       " 'collegeag',\n",
       " 'daughter',\n",
       " 'traci',\n",
       " 'daughter',\n",
       " 'via',\n",
       " 'first',\n",
       " 'marriag',\n",
       " 'therefor',\n",
       " 'ellen',\n",
       " 'real',\n",
       " 'daughter',\n",
       " 'insist',\n",
       " 'read',\n",
       " 'letter',\n",
       " 'address',\n",
       " 'traci',\n",
       " 'loud',\n",
       " 'accus',\n",
       " 'place',\n",
       " 'hand',\n",
       " 'privat',\n",
       " 'place',\n",
       " 'respond',\n",
       " 'wife',\n",
       " 'question',\n",
       " 'want',\n",
       " 'know',\n",
       " 'true',\n",
       " 'respond',\n",
       " 'wonder',\n",
       " 'traci',\n",
       " 'sai',\n",
       " 'probabl',\n",
       " 'mix',\n",
       " 'result',\n",
       " 'apocalypt',\n",
       " 'film',\n",
       " 'becom',\n",
       " 'disturbingli',\n",
       " 'mysteri',\n",
       " 'evas',\n",
       " 'never',\n",
       " 'settl',\n",
       " 'sure',\n",
       " 'tell',\n",
       " 'truth',\n",
       " 'destroi',\n",
       " 'famili',\n",
       " 'scene',\n",
       " 'could',\n",
       " 'deem',\n",
       " 'attack',\n",
       " 'america',\n",
       " 'soul',\n",
       " 'expos',\n",
       " 'question',\n",
       " 'truth',\n",
       " 'charact',\n",
       " 'principl',\n",
       " 'put',\n",
       " 'microscop',\n",
       " 'stori',\n",
       " 'build',\n",
       " 'tragic',\n",
       " 'outcome\\nthi',\n",
       " 'on',\n",
       " 'jost',\n",
       " 'deepest',\n",
       " 'penetr',\n",
       " 'film',\n",
       " 'could',\n",
       " 'even',\n",
       " 'argu',\n",
       " 'made',\n",
       " 'classic',\n",
       " 'film',\n",
       " 'forcefulli',\n",
       " 'subtli',\n",
       " 'tell',\n",
       " 'american',\n",
       " 'stori',\n",
       " 'replet',\n",
       " 'unansw',\n",
       " 'question',\n",
       " 'famili',\n",
       " 'life',\n",
       " 'haunt',\n",
       " 'give',\n",
       " 'film',\n",
       " 'certain',\n",
       " 'power',\n",
       " 'make',\n",
       " 'think',\n",
       " 'long',\n",
       " 'time',\n",
       " 'afterward',\n",
       " 'countri',\n",
       " 'raw',\n",
       " 'violent',\n",
       " 'natur',\n",
       " 'becom',\n",
       " 'part',\n",
       " 'peopl',\n",
       " 'nature\\non',\n",
       " 'scene',\n",
       " 'found',\n",
       " 'memor',\n",
       " 'camera',\n",
       " 'pan',\n",
       " 'diner',\n",
       " 'tom',\n",
       " 'dine',\n",
       " 'cowork',\n",
       " 'could',\n",
       " 'hear',\n",
       " 'first',\n",
       " 'muffl',\n",
       " 'convers',\n",
       " 'patron',\n",
       " 'camera',\n",
       " 'meticul',\n",
       " 'pan',\n",
       " 'diner',\n",
       " 'atmospher',\n",
       " 'place',\n",
       " 'fulli',\n",
       " 'absorb',\n",
       " 'return',\n",
       " 'tom',\n",
       " 'convers',\n",
       " 'becam',\n",
       " 'clearer',\n",
       " 'scene',\n",
       " 'plai',\n",
       " 'daili',\n",
       " 'experi',\n",
       " 'american',\n",
       " 'rare',\n",
       " 'captur',\n",
       " 'exactli',\n",
       " 'film',\n",
       " 'time',\n",
       " 'consum',\n",
       " 'shot',\n",
       " 'attempt',\n",
       " 'commerci',\n",
       " 'filmmak',\n",
       " 'live',\n",
       " 'fear',\n",
       " 'lose',\n",
       " 'audienc',\n",
       " 'long',\n",
       " 'nonact',\n",
       " 'shotthi',\n",
       " 'on',\n",
       " 'jost',\n",
       " 'strong',\n",
       " 'point',\n",
       " 'willing',\n",
       " 'explor',\n",
       " 'territori',\n",
       " 'other',\n",
       " 'fear',\n",
       " 'go\\njost',\n",
       " 'film',\n",
       " 'justifi',\n",
       " 'critic',\n",
       " 'laps',\n",
       " 'stori',\n",
       " 'line',\n",
       " 'didnt',\n",
       " 'clarifi',\n",
       " 'precis',\n",
       " 'importantli',\n",
       " 'prais',\n",
       " 'poetri',\n",
       " 'bring',\n",
       " 'stori',\n",
       " 'tell',\n",
       " 'malais',\n",
       " 'american',\n",
       " 'cultur',\n",
       " 'difficult',\n",
       " 'grip',\n",
       " 'american',\n",
       " 'landscap',\n",
       " 'perceiv',\n",
       " 'beauti',\n",
       " 'sight',\n",
       " 'behold',\n",
       " 'countri',\n",
       " 'wealthi',\n",
       " 'place',\n",
       " 'compar',\n",
       " 'rest',\n",
       " 'world',\n",
       " 'ask',\n",
       " 'mean',\n",
       " 'american',\n",
       " 'happi',\n",
       " 'people\\nreview',\n",
       " '\\ndenni',\n",
       " 'schwartz',\n",
       " 'movi',\n",
       " 'reviews\\na',\n",
       " 'right',\n",
       " 'reserv',\n",
       " 'denni',\n",
       " 'schwartz\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the corpus of news to a document term matrix (dtm), where each row represents a document, each column represents the number of occurence for a word\n",
    "\n",
    "dtm = DocumentTermMatrix(corpus, docids = filelists.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16748.txt': 3.0,\n",
       " '17108.txt': 3.0,\n",
       " '17109.txt': 1.0,\n",
       " '17110.txt': 2.0,\n",
       " '17111.txt': 1.0,\n",
       " '17116.txt': 1.0,\n",
       " '17117.txt': 5.0,\n",
       " '17118.txt': 1.0,\n",
       " '17119.txt': 5.0,\n",
       " '17139.txt': 1.0,\n",
       " '17144.txt': 1.0,\n",
       " '17145.txt': 2.0,\n",
       " '17146.txt': 2.0,\n",
       " '17147.txt': 1.0,\n",
       " '17150.txt': 1.0,\n",
       " '17185.txt': 3.0,\n",
       " '17192.txt': 4.0,\n",
       " '17219.txt': 1.0,\n",
       " '17239.txt': 2.0,\n",
       " '17243.txt': 1.0,\n",
       " '17254.txt': 1.0,\n",
       " '17255.txt': 1.0,\n",
       " '17280.txt': 2.0,\n",
       " '17300.txt': 2.0,\n",
       " '17303.txt': 2.0,\n",
       " '17341.txt': 1.0,\n",
       " '17384.txt': 2.0,\n",
       " '17398.txt': 1.0,\n",
       " '17399.txt': 3.0,\n",
       " '17430.txt': 2.0,\n",
       " '17431.txt': 2.0,\n",
       " '17447.txt': 1.0,\n",
       " '17457.txt': 1.0,\n",
       " '17460.txt': 2.0,\n",
       " '17501.txt': 1.0,\n",
       " '17518.txt': 1.0,\n",
       " '17534.txt': 1.0,\n",
       " '17578.txt': 1.0,\n",
       " '17609.txt': 1.0,\n",
       " '17610.txt': 1.0,\n",
       " '17655.txt': 1.0,\n",
       " '17663.txt': 1.0,\n",
       " '17695.txt': 2.0,\n",
       " '17711.txt': 1.0,\n",
       " '17713.txt': 2.0,\n",
       " '17753.txt': 1.0,\n",
       " '17757.txt': 2.0,\n",
       " '17758.txt': 2.0,\n",
       " '17761.txt': 1.0,\n",
       " '17803.txt': 1.0,\n",
       " '17811.txt': 1.0,\n",
       " '17874.txt': 2.0,\n",
       " '17879.txt': 1.0,\n",
       " '17886.txt': 3.0,\n",
       " '17896.txt': 1.0,\n",
       " '17898.txt': 1.0,\n",
       " '17902.txt': 1.0,\n",
       " '17912.txt': 2.0,\n",
       " '17933.txt': 2.0,\n",
       " '17934.txt': 1.0,\n",
       " '17945.txt': 2.0,\n",
       " '17963.txt': 2.0,\n",
       " '17971.txt': 3.0,\n",
       " '17992.txt': 1.0,\n",
       " '18004.txt': 1.0,\n",
       " '18016.txt': 1.0,\n",
       " '18032.txt': 1.0,\n",
       " '18067.txt': 1.0,\n",
       " '18068.txt': 3.0,\n",
       " '18080.txt': 3.0,\n",
       " '18087.txt': 3.0,\n",
       " '18088.txt': 2.0,\n",
       " '18136.txt': 1.0,\n",
       " '18141.txt': 1.0,\n",
       " '18156.txt': 2.0,\n",
       " '18161.txt': 1.0,\n",
       " '18181.txt': 4.0,\n",
       " '18227.txt': 1.0,\n",
       " '18263.txt': 2.0,\n",
       " '18272.txt': 4.0,\n",
       " '18273.txt': 2.0,\n",
       " '18274.txt': 1.0,\n",
       " '18282.txt': 1.0,\n",
       " '18283.txt': 1.0,\n",
       " '18307.txt': 8.0,\n",
       " '18368.txt': 3.0,\n",
       " '18376.txt': 2.0,\n",
       " '18396.txt': 1.0,\n",
       " '18406.txt': 2.0,\n",
       " '18413.txt': 2.0,\n",
       " '18414.txt': 1.0,\n",
       " '18447.txt': 3.0,\n",
       " '18473.txt': 1.0,\n",
       " '18480.txt': 1.0,\n",
       " '18485.txt': 1.0,\n",
       " '18498.txt': 3.0,\n",
       " '1858.txt': 1.0,\n",
       " '1859.txt': 1.0,\n",
       " '1860.txt': 4.0,\n",
       " '1864.txt': 1.0,\n",
       " '1865.txt': 1.0,\n",
       " '1866.txt': 1.0,\n",
       " '1867.txt': 1.0,\n",
       " '1889.txt': 1.0,\n",
       " '1891.txt': 1.0,\n",
       " '1908.txt': 1.0,\n",
       " '1910.txt': 1.0,\n",
       " '1911.txt': 1.0,\n",
       " '1912.txt': 1.0,\n",
       " '1917.txt': 3.0,\n",
       " '1921.txt': 1.0,\n",
       " '1925.txt': 1.0,\n",
       " '1928.txt': 1.0,\n",
       " '1929.txt': 1.0,\n",
       " '1930.txt': 1.0,\n",
       " '1932.txt': 1.0,\n",
       " '1944.txt': 2.0,\n",
       " '1945.txt': 2.0,\n",
       " '1961.txt': 2.0,\n",
       " '1967.txt': 1.0,\n",
       " '1968.txt': 1.0,\n",
       " '1974.txt': 1.0,\n",
       " '1975.txt': 2.0,\n",
       " '1976.txt': 3.0,\n",
       " '1981.txt': 2.0,\n",
       " '1984.txt': 1.0,\n",
       " '1985.txt': 1.0,\n",
       " '1990.txt': 3.0,\n",
       " '1994.txt': 1.0,\n",
       " '2005.txt': 1.0,\n",
       " '2006.txt': 1.0,\n",
       " '2007.txt': 1.0,\n",
       " '2008.txt': 1.0,\n",
       " '2009.txt': 1.0,\n",
       " '2025.txt': 2.0,\n",
       " '2026.txt': 1.0,\n",
       " '2030.txt': 2.0,\n",
       " '2031.txt': 1.0,\n",
       " '2036.txt': 2.0,\n",
       " '2045.txt': 1.0,\n",
       " '2046.txt': 1.0,\n",
       " '2051.txt': 1.0,\n",
       " '2055.txt': 1.0,\n",
       " '2058.txt': 1.0,\n",
       " '2059.txt': 1.0,\n",
       " '2062.txt': 2.0,\n",
       " '2076.txt': 3.0,\n",
       " '2080.txt': 1.0,\n",
       " '2081.txt': 1.0,\n",
       " '2085.txt': 1.0,\n",
       " '2086.txt': 1.0,\n",
       " '2087.txt': 2.0,\n",
       " '2089.txt': 1.0,\n",
       " '2090.txt': 1.0,\n",
       " '2091.txt': 1.0,\n",
       " '2094.txt': 1.0,\n",
       " '2095.txt': 1.0,\n",
       " '2098.txt': 1.0,\n",
       " '2099.txt': 3.0,\n",
       " '2110.txt': 1.0,\n",
       " '2113.txt': 1.0,\n",
       " '2115.txt': 1.0}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of occurence of the word \"director\" in each document\n",
    "dtm.get_token_occurences('director')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dtm.get_token_occurences('director'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to calculate entropy given a distribution\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def entropy(p):\n",
    "    if sum(p) == 0:\n",
    "        return 0\n",
    "\n",
    "    p = p/sum(p)\n",
    "\n",
    "    p = p[ p > 0 ]\n",
    "\n",
    "    H = -sum(p*np.log2(p))\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7424875695421236"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(np.array( [12,45]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854752972273344"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy( np.array( [100,60,40]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['16748.txt', '17108.txt', '17109.txt', '17110.txt', '17111.txt', '17116.txt', '17117.txt', '17118.txt', '17119.txt', '17139.txt', '17144.txt', '17145.txt', '17146.txt', '17147.txt', '17150.txt', '17185.txt', '17192.txt', '17219.txt', '17239.txt', '17243.txt', '17254.txt', '17255.txt', '17280.txt', '17300.txt', '17303.txt', '17341.txt', '17384.txt', '17398.txt', '17399.txt', '17430.txt', '17431.txt', '17447.txt', '17457.txt', '17460.txt', '17501.txt', '17518.txt', '17534.txt', '17578.txt', '17609.txt', '17610.txt', '17655.txt', '17663.txt', '17695.txt', '17711.txt', '17713.txt', '17753.txt', '17757.txt', '17758.txt', '17761.txt', '17803.txt', '17811.txt', '17874.txt', '17879.txt', '17886.txt', '17896.txt', '17898.txt', '17902.txt', '17912.txt', '17933.txt', '17934.txt', '17945.txt', '17963.txt', '17971.txt', '17992.txt', '18004.txt', '18016.txt', '18032.txt', '18067.txt', '18068.txt', '18080.txt', '18087.txt', '18088.txt', '18136.txt', '18141.txt', '18156.txt', '18161.txt', '18181.txt', '18227.txt', '18263.txt', '18272.txt', '18273.txt', '18274.txt', '18282.txt', '18283.txt', '18307.txt', '18368.txt', '18376.txt', '18396.txt', '18406.txt', '18413.txt', '18414.txt', '18447.txt', '18473.txt', '18480.txt', '18485.txt', '18498.txt', '1858.txt', '1859.txt', '1860.txt', '1864.txt', '1865.txt', '1866.txt', '1867.txt', '1889.txt', '1891.txt', '1908.txt', '1910.txt', '1911.txt', '1912.txt', '1917.txt', '1921.txt', '1925.txt', '1928.txt', '1929.txt', '1930.txt', '1932.txt', '1944.txt', '1945.txt', '1961.txt', '1967.txt', '1968.txt', '1974.txt', '1975.txt', '1976.txt', '1981.txt', '1984.txt', '1985.txt', '1990.txt', '1994.txt', '2005.txt', '2006.txt', '2007.txt', '2008.txt', '2009.txt', '2025.txt', '2026.txt', '2030.txt', '2031.txt', '2036.txt', '2045.txt', '2046.txt', '2051.txt', '2055.txt', '2058.txt', '2059.txt', '2062.txt', '2076.txt', '2080.txt', '2081.txt', '2085.txt', '2086.txt', '2087.txt', '2089.txt', '2090.txt', '2091.txt', '2094.txt', '2095.txt', '2098.txt', '2099.txt', '2110.txt', '2113.txt', '2115.txt'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm.get_token_occurences('director').keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16748.txt',\n",
       " '17108.txt',\n",
       " '17109.txt',\n",
       " '17110.txt',\n",
       " '17111.txt',\n",
       " '17116.txt',\n",
       " '17117.txt',\n",
       " '17118.txt',\n",
       " '17119.txt',\n",
       " '17139.txt',\n",
       " '17144.txt',\n",
       " '17145.txt',\n",
       " '17146.txt',\n",
       " '17147.txt',\n",
       " '17150.txt',\n",
       " '17185.txt',\n",
       " '17192.txt',\n",
       " '17219.txt',\n",
       " '17239.txt',\n",
       " '17243.txt',\n",
       " '17254.txt',\n",
       " '17255.txt',\n",
       " '17280.txt',\n",
       " '17300.txt',\n",
       " '17303.txt',\n",
       " '17341.txt',\n",
       " '17384.txt',\n",
       " '17398.txt',\n",
       " '17399.txt',\n",
       " '17430.txt',\n",
       " '17431.txt',\n",
       " '17447.txt',\n",
       " '17457.txt',\n",
       " '17460.txt',\n",
       " '17501.txt',\n",
       " '17518.txt',\n",
       " '17534.txt',\n",
       " '17578.txt',\n",
       " '17609.txt',\n",
       " '17610.txt',\n",
       " '17655.txt',\n",
       " '17663.txt',\n",
       " '17695.txt',\n",
       " '17711.txt',\n",
       " '17713.txt',\n",
       " '17753.txt',\n",
       " '17757.txt',\n",
       " '17758.txt',\n",
       " '17761.txt',\n",
       " '17803.txt',\n",
       " '17811.txt',\n",
       " '17874.txt',\n",
       " '17879.txt',\n",
       " '17886.txt',\n",
       " '17896.txt',\n",
       " '17898.txt',\n",
       " '17902.txt',\n",
       " '17912.txt',\n",
       " '17933.txt',\n",
       " '17934.txt',\n",
       " '17945.txt',\n",
       " '17963.txt',\n",
       " '17971.txt',\n",
       " '17992.txt',\n",
       " '18004.txt',\n",
       " '18016.txt',\n",
       " '18032.txt',\n",
       " '18067.txt',\n",
       " '18068.txt',\n",
       " '18080.txt',\n",
       " '18087.txt',\n",
       " '18088.txt',\n",
       " '18136.txt',\n",
       " '18141.txt',\n",
       " '18156.txt',\n",
       " '18161.txt',\n",
       " '18181.txt',\n",
       " '18227.txt',\n",
       " '18263.txt',\n",
       " '18272.txt',\n",
       " '18273.txt',\n",
       " '18274.txt',\n",
       " '18282.txt',\n",
       " '18283.txt',\n",
       " '18307.txt',\n",
       " '18368.txt',\n",
       " '18376.txt',\n",
       " '18396.txt',\n",
       " '18406.txt',\n",
       " '18413.txt',\n",
       " '18414.txt',\n",
       " '18447.txt',\n",
       " '18473.txt',\n",
       " '18480.txt',\n",
       " '18485.txt',\n",
       " '18498.txt',\n",
       " '1858.txt',\n",
       " '1859.txt',\n",
       " '1860.txt',\n",
       " '1864.txt',\n",
       " '1865.txt',\n",
       " '1866.txt',\n",
       " '1867.txt',\n",
       " '1889.txt',\n",
       " '1891.txt',\n",
       " '1908.txt',\n",
       " '1910.txt',\n",
       " '1911.txt',\n",
       " '1912.txt',\n",
       " '1917.txt',\n",
       " '1921.txt',\n",
       " '1925.txt',\n",
       " '1928.txt',\n",
       " '1929.txt',\n",
       " '1930.txt',\n",
       " '1932.txt',\n",
       " '1944.txt',\n",
       " '1945.txt',\n",
       " '1961.txt',\n",
       " '1967.txt',\n",
       " '1968.txt',\n",
       " '1974.txt',\n",
       " '1975.txt',\n",
       " '1976.txt',\n",
       " '1981.txt',\n",
       " '1984.txt',\n",
       " '1985.txt',\n",
       " '1990.txt',\n",
       " '1994.txt',\n",
       " '2005.txt',\n",
       " '2006.txt',\n",
       " '2007.txt',\n",
       " '2008.txt',\n",
       " '2009.txt',\n",
       " '2025.txt',\n",
       " '2026.txt',\n",
       " '2030.txt',\n",
       " '2031.txt',\n",
       " '2036.txt',\n",
       " '2045.txt',\n",
       " '2046.txt',\n",
       " '2051.txt',\n",
       " '2055.txt',\n",
       " '2058.txt',\n",
       " '2059.txt',\n",
       " '2062.txt',\n",
       " '2076.txt',\n",
       " '2080.txt',\n",
       " '2081.txt',\n",
       " '2085.txt',\n",
       " '2086.txt',\n",
       " '2087.txt',\n",
       " '2089.txt',\n",
       " '2090.txt',\n",
       " '2091.txt',\n",
       " '2094.txt',\n",
       " '2095.txt',\n",
       " '2098.txt',\n",
       " '2099.txt',\n",
       " '2110.txt',\n",
       " '2113.txt',\n",
       " '2115.txt']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the documents that contain the word \"director\" from dictionary keys to a list\n",
    "director_docs = list(dtm.get_token_occurences('director').keys())\n",
    "director_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1912.txt', '1866.txt', '1867.txt', '18087.txt', '1911.txt', '1859.txt', '1865.txt', '1864.txt', '1858.txt', '17610.txt', '1910.txt', '18480.txt', '17758.txt', '18447.txt', '1928.txt', '18080.txt', '1860.txt', '18282.txt', '18283.txt', '18136.txt', '2005.txt', '17239.txt', '18485.txt', '1929.txt', '17761.txt', '1917.txt', '2007.txt', '18068.txt', '17398.txt', '1889.txt', '17399.txt', '2006.txt', '17945.txt', '1916.txt', '17992.txt', '17711.txt', '18032.txt', '2115.txt', '2101.txt', '17117.txt', '17116.txt', '17300.txt', '2076.txt', '2062.txt', '2089.txt', '18227.txt', '17896.txt', '17303.txt', '17934.txt', '1967.txt', '17713.txt', '1998.txt', '2067.txt', '17501.txt', '2098.txt', '2113.txt', '17139.txt', '17111.txt', '17663.txt', '17879.txt', '17662.txt', '17110.txt', '17886.txt', '17460.txt', '2099.txt', '18141.txt', '1976.txt', '17933.txt', '1974.txt', '18396.txt', '2058.txt', '2110.txt', '18181.txt', '18156.txt', '2059.txt', '1961.txt', '1975.txt', '18368.txt', '1944.txt', '17532.txt', '17254.txt', '2108.txt', '17447.txt', '2055.txt', '17255.txt', '17902.txt', '1945.txt', '1979.txt', '18414.txt', '1984.txt', '1990.txt', '17243.txt', '2043.txt', '17280.txt', '2094.txt', '2080.txt', '18004.txt', '17109.txt', '17874.txt', '17108.txt', '2081.txt', '2095.txt', '17518.txt', '1991.txt', '1985.txt', '1981.txt', '2046.txt', '17534.txt', '18161.txt', '2091.txt', '2085.txt', '17118.txt', '17695.txt', '17119.txt', '2090.txt', '1943.txt', '18376.txt', '1994.txt', '18406.txt', '17912.txt', '2045.txt', '2051.txt', '2079.txt', '2086.txt', '18016.txt', '17457.txt', '17655.txt', '17898.txt', '16748.txt', '2087.txt', '1968.txt', '18375.txt', '18413.txt', '17753.txt', '17431.txt', '18272.txt', '1891.txt', '18273.txt', '17430.txt', '17578.txt', '2036.txt', '1932.txt', '18307.txt', '17963.txt', '1930.txt', '2008.txt', '18067.txt', '17803.txt', '17341.txt', '2035.txt', '2009.txt', '1925.txt', '1921.txt', '17219.txt', '2031.txt', '2025.txt', '17609.txt', '17147.txt', '18274.txt', '17185.txt', '17146.txt', '18088.txt', '2030.txt', '1908.txt', '18498.txt', '1934.txt', '18473.txt', '17971.txt', '2026.txt', '17391.txt', '17144.txt', '18263.txt', '17150.txt', '17811.txt', '17192.txt', '17145.txt', '17384.txt', '2033.txt', '1937.txt', '17757.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the current working directory\n",
    "cwd = '/Users/jakebrulato/Documents/GitHub/KDD/Homework_7/MovieReviews'\n",
    "\n",
    "# Create a list to store the file names\n",
    "file_names = []\n",
    "\n",
    "# Loop through the current directory\n",
    "for file in os.listdir(cwd):\n",
    "\n",
    "    # Check if the file is a txt file\n",
    "    if file.endswith(\".txt\"):\n",
    "\n",
    "        # Add the file name to the list\n",
    "        file_names.append(file)\n",
    "\n",
    "# Print the list of file names\n",
    "print(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'in <string>' requires string as left operand, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m count_dentistry \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m director_docs:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mfile_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m:\n\u001b[1;32m      5\u001b[0m         count_dentistry \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(count_dentistry)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'in <string>' requires string as left operand, not list"
     ]
    }
   ],
   "source": [
    "#print the number of documents that contain the word \"Director\"\n",
    "count_dentistry = 0\n",
    "for item in director_docs:\n",
    "    if file_names in item:\n",
    "        count_dentistry += 1\n",
    "print(count_dentistry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
