{"cells":[{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20664,"status":"ok","timestamp":1696526173899,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"QiA2YsDSVzYW","outputId":"61201b79-946f-4acb-e6b7-b90523818b5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scipy==1.10.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (1.10.0)\n","Requirement already satisfied: numpy==1.23.5 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (1.23.5)\n"]}],"source":["!pip install -U scipy==1.10.0 numpy==1.23.5"]},{"cell_type":"code","execution_count":81,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84110,"status":"ok","timestamp":1696526260805,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"SkHF5qsY3zNM","outputId":"7d983dc6-fad1-4bb3-ed70-e8c5e930d037"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/stephenhky/PyShortTextCategorization\n","  Cloning https://github.com/stephenhky/PyShortTextCategorization to c:\\users\\rbrul\\appdata\\local\\temp\\pip-req-build-hkb93jc2\n","  Resolved https://github.com/stephenhky/PyShortTextCategorization to commit e183bac2a362051087e3cb9160ccd8f2ee67f315\n","  Installing build dependencies: started\n","  Installing build dependencies: finished with status 'done'\n","  Getting requirements to build wheel: started\n","  Getting requirements to build wheel: finished with status 'done'\n","  Installing backend dependencies: started\n","  Installing backend dependencies: finished with status 'done'\n","  Preparing metadata (pyproject.toml): started\n","  Preparing metadata (pyproject.toml): finished with status 'done'\n","Requirement already satisfied: Cython>=3.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (3.0.9)\n","Requirement already satisfied: numpy>=1.23.3 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (1.23.5)\n","Requirement already satisfied: scipy>=1.10.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (1.10.0)\n","Requirement already satisfied: joblib>=1.3.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (1.3.2)\n","Requirement already satisfied: scikit-learn>=1.2.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (1.4.1.post1)\n","Requirement already satisfied: tensorflow>=2.13.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (2.15.0)\n","Requirement already satisfied: keras>=2.13.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (2.15.0)\n","Requirement already satisfied: gensim>=4.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (4.3.2)\n","Requirement already satisfied: pandas>=1.2.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (2.2.1)\n","Requirement already satisfied: snowballstemmer>=2.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (2.2.0)\n","Requirement already satisfied: transformers>=4.32.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (4.38.2)\n","Requirement already satisfied: torch>=2.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (2.2.1)\n","Requirement already satisfied: python-Levenshtein>=0.21.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (0.25.0)\n","Requirement already satisfied: numba>=0.57.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from shorttext==1.6.1) (0.59.0)\n","Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from gensim>=4.0.0->shorttext==1.6.1) (7.0.1)\n","Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from numba>=0.57.0->shorttext==1.6.1) (0.42.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from pandas>=1.2.0->shorttext==1.6.1) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from pandas>=1.2.0->shorttext==1.6.1) (2024.1)\n","Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from pandas>=1.2.0->shorttext==1.6.1) (2024.1)\n","Requirement already satisfied: Levenshtein==0.25.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from python-Levenshtein>=0.21.0->shorttext==1.6.1) (0.25.0)\n","Requirement already satisfied: rapidfuzz<4.0.0,>=3.1.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from Levenshtein==0.25.0->python-Levenshtein>=0.21.0->shorttext==1.6.1) (3.6.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from scikit-learn>=1.2.0->shorttext==1.6.1) (3.3.0)\n","Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow>=2.13.0->shorttext==1.6.1) (2.15.0)\n","Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (2.1.0)\n","Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (24.3.7)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (3.10.0)\n","Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (16.0.6)\n","Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (3.3.0)\n","Requirement already satisfied: packaging in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (4.25.3)\n","Requirement already satisfied: setuptools in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (57.4.0)\n","Requirement already satisfied: six>=1.12.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (2.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (4.10.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.31.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (1.62.0)\n","Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (2.15.2)\n","Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (2.15.0)\n","Requirement already satisfied: filelock in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from torch>=2.0.0->shorttext==1.6.1) (3.13.1)\n","Requirement already satisfied: sympy in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from torch>=2.0.0->shorttext==1.6.1) (1.12)\n","Requirement already satisfied: networkx in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from torch>=2.0.0->shorttext==1.6.1) (3.2.1)\n","Requirement already satisfied: jinja2 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from torch>=2.0.0->shorttext==1.6.1) (3.1.3)\n","Requirement already satisfied: fsspec in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from torch>=2.0.0->shorttext==1.6.1) (2024.2.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from transformers>=4.32.0->shorttext==1.6.1) (0.21.4)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from transformers>=4.32.0->shorttext==1.6.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from transformers>=4.32.0->shorttext==1.6.1) (2023.12.25)\n","Requirement already satisfied: requests in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from transformers>=4.32.0->shorttext==1.6.1) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from transformers>=4.32.0->shorttext==1.6.1) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from transformers>=4.32.0->shorttext==1.6.1) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from transformers>=4.32.0->shorttext==1.6.1) (4.66.2)\n","Requirement already satisfied: colorama in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers>=4.32.0->shorttext==1.6.1) (0.4.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->shorttext==1.6.1) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from requests->transformers>=4.32.0->shorttext==1.6.1) (2024.2.2)\n","Requirement already satisfied: mpmath>=0.19 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from sympy->torch>=2.0.0->shorttext==1.6.1) (1.3.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.42.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (2.28.1)\n","Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (1.2.0)\n","Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (3.5.2)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (5.3.3)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (1.3.1)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (0.5.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\rbrul\\documents\\github\\kdd\\.venv\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow>=2.13.0->shorttext==1.6.1) (3.2.2)\n"]},{"name":"stderr","output_type":"stream","text":["  Running command git clone --filter=blob:none --quiet https://github.com/stephenhky/PyShortTextCategorization 'C:\\Users\\rbrul\\AppData\\Local\\Temp\\pip-req-build-hkb93jc2'\n"]}],"source":["!pip install -U git+https://github.com/stephenhky/PyShortTextCategorization"]},{"cell_type":"code","execution_count":82,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21674,"status":"ok","timestamp":1696526312062,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"RgNO7cxY339l","outputId":"9c0a3fbc-abca-4d0a-c895-ada906204247"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\rbrul\\Documents\\GitHub\\KDD\\.venv\\Scripts\\python.exe: No module named spacy\n"]}],"source":["!python -m spacy download en"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2397,"status":"ok","timestamp":1696526383089,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"saiHTVjS11nm","outputId":"d246a4f7-b0df-4473-f86f-173be175aa1d"},"outputs":[{"data":{"text/plain":["['16748.txt',\n"," '17108.txt',\n"," '17109.txt',\n"," '17110.txt',\n"," '17111.txt',\n"," '17116.txt',\n"," '17117.txt',\n"," '17118.txt',\n"," '17119.txt',\n"," '17139.txt',\n"," '17144.txt',\n"," '17145.txt',\n"," '17146.txt',\n"," '17147.txt',\n"," '17150.txt',\n"," '17185.txt',\n"," '17192.txt',\n"," '17219.txt',\n"," '17239.txt',\n"," '17243.txt',\n"," '17254.txt',\n"," '17255.txt',\n"," '17280.txt',\n"," '17300.txt',\n"," '17303.txt',\n"," '17341.txt',\n"," '17384.txt',\n"," '17391.txt',\n"," '17398.txt',\n"," '17399.txt',\n"," '17430.txt',\n"," '17431.txt',\n"," '17447.txt',\n"," '17457.txt',\n"," '17460.txt',\n"," '17501.txt',\n"," '17518.txt',\n"," '17532.txt',\n"," '17534.txt',\n"," '17578.txt',\n"," '17609.txt',\n"," '17610.txt',\n"," '17655.txt',\n"," '17662.txt',\n"," '17663.txt',\n"," '17695.txt',\n"," '17711.txt',\n"," '17713.txt',\n"," '17753.txt',\n"," '17757.txt',\n"," '17758.txt',\n"," '17761.txt',\n"," '17803.txt',\n"," '17811.txt',\n"," '17874.txt',\n"," '17879.txt',\n"," '17886.txt',\n"," '17896.txt',\n"," '17898.txt',\n"," '17902.txt',\n"," '17912.txt',\n"," '17933.txt',\n"," '17934.txt',\n"," '17945.txt',\n"," '17963.txt',\n"," '17971.txt',\n"," '17992.txt',\n"," '18004.txt',\n"," '18016.txt',\n"," '18032.txt',\n"," '18067.txt',\n"," '18068.txt',\n"," '18080.txt',\n"," '18087.txt',\n"," '18088.txt',\n"," '18136.txt',\n"," '18141.txt',\n"," '18156.txt',\n"," '18161.txt',\n"," '18181.txt',\n"," '18227.txt',\n"," '18263.txt',\n"," '18272.txt',\n"," '18273.txt',\n"," '18274.txt',\n"," '18282.txt',\n"," '18283.txt',\n"," '18307.txt',\n"," '18368.txt',\n"," '18375.txt',\n"," '18376.txt',\n"," '18396.txt',\n"," '18406.txt',\n"," '18413.txt',\n"," '18414.txt',\n"," '18447.txt',\n"," '18473.txt',\n"," '18480.txt',\n"," '18485.txt',\n"," '18498.txt',\n"," '1858.txt',\n"," '1859.txt',\n"," '1860.txt',\n"," '1864.txt',\n"," '1865.txt',\n"," '1866.txt',\n"," '1867.txt',\n"," '1889.txt',\n"," '1891.txt',\n"," '1908.txt',\n"," '1910.txt',\n"," '1911.txt',\n"," '1912.txt',\n"," '1916.txt',\n"," '1917.txt',\n"," '1921.txt',\n"," '1925.txt',\n"," '1928.txt',\n"," '1929.txt',\n"," '1930.txt',\n"," '1932.txt',\n"," '1934.txt',\n"," '1937.txt',\n"," '1943.txt',\n"," '1944.txt',\n"," '1945.txt',\n"," '1961.txt',\n"," '1967.txt',\n"," '1968.txt',\n"," '1974.txt',\n"," '1975.txt',\n"," '1976.txt',\n"," '1979.txt',\n"," '1981.txt',\n"," '1984.txt',\n"," '1985.txt',\n"," '1990.txt',\n"," '1991.txt',\n"," '1994.txt',\n"," '1998.txt',\n"," '2005.txt',\n"," '2006.txt',\n"," '2007.txt',\n"," '2008.txt',\n"," '2009.txt',\n"," '2025.txt',\n"," '2026.txt',\n"," '2030.txt',\n"," '2031.txt',\n"," '2033.txt',\n"," '2035.txt',\n"," '2036.txt',\n"," '2043.txt',\n"," '2045.txt',\n"," '2046.txt',\n"," '2051.txt',\n"," '2055.txt',\n"," '2058.txt',\n"," '2059.txt',\n"," '2062.txt',\n"," '2067.txt',\n"," '2076.txt',\n"," '2079.txt',\n"," '2080.txt',\n"," '2081.txt',\n"," '2085.txt',\n"," '2086.txt',\n"," '2087.txt',\n"," '2089.txt',\n"," '2090.txt',\n"," '2091.txt',\n"," '2094.txt',\n"," '2095.txt',\n"," '2098.txt',\n"," '2099.txt',\n"," '2101.txt',\n"," '2108.txt',\n"," '2110.txt',\n"," '2113.txt',\n"," '2115.txt']"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["from nltk.corpus import PlaintextCorpusReader\n","corpus_root = 'MovieReviews' # Folder Name\n","filelists = PlaintextCorpusReader(corpus_root, '.*',encoding='latin-1')  # wildcard is read all files in the folder\n","filelists.fileids()  # Get the filenames"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["corpus_root = 'MovieReviews'\n","bern_file_count: int = 80"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total files: 180 Berardinelli files: 80 Schwartz files: 100\n"]}],"source":["files: PlaintextCorpusReader = PlaintextCorpusReader(corpus_root, '.*', encoding='latin-1')\n","\n","bern_files: list = files.fileids()[:bern_file_count]\n","sch_files: list = files.fileids()[bern_file_count:]\n","\n","bern_file_count = len(bern_files)\n","sch_file_count: int = len(sch_files)\n","total_file_count: int = bern_file_count + sch_file_count\n","\n","print('Total files:', len(files.fileids()), 'Berardinelli files:', bern_file_count, \n","      'Schwartz files:', sch_file_count)"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":2047,"status":"ok","timestamp":1696526402457,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"gEITAV0z11ns"},"outputs":[],"source":["#create a list of news articles\n","news = []\n","for fileid in filelists.fileids():\n","    news.append(filelists.raw(fileid))"]},{"cell_type":"code","execution_count":87,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":89},"executionInfo":{"elapsed":126,"status":"ok","timestamp":1696526424884,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"wkMn62EY11ns","outputId":"13c0e0e2-43bd-4485-a621-ba53cbbbf223"},"outputs":[{"data":{"text/plain":["'THE BED YOU SLEEP IN (director: Jon Jost; cast: Ray Weiss (Tom Blair), Doug (Marshall Gaddis), Beth (Kate Sannella), Scott (Brad Shelton), Mrs. Weiss (Ellen McLaughlin), 1993)\\r\\nTruly independent filmmaker, Jost, has completed his so-called trilogy about rural America with this film and has since moved on to self- imposed exile by going to Europe. This extra-ordinary film offers a long hard look at its subject matter, as the camera is held steadfast, not moving for insatiably long periods of time, picking up all the appropriate nuances it needs to with deliberate dispassion, as it looks at an Oregon lumber mill, whose owner (Tom Blair) is faced with unsettling economic news about the business he has built-up and worked at for his 50- odd years of life. It focuses on this man and tries to find out who he is, using him as a metaphoric symbol for America, perhaps, emulating Emerson\\'s views, as his writings are flashed on screen, exhibiting some sayings from his essays on nature and America.\\r\\nBy seeing who this man is, we get to see how he adjusts to his carefully scripted life, the fly-fishing he loves to do for the sport of it, his easy and almost gentile manners, and his very definite American persona, as he is forced out of economic necessity to deal with the Japanese businessmen he inherently despises, and we get a picture of a rather complicated individual, who has difficulty in communicating with himself and others, so the closer we get to him, the more we sense that there are a lot of things about him that remain unanswered. The shocker about his life that is about to unfold, comes after he meets a foreign stranger on the street, raving about the day of atonement coming soon and how God knows all, that he should pray with him, but is told by him that he has no time for that, as he feels uncomfortable being around this religious zealot, so he fumbles around with his wad of bills and thrusts a few dollars in the preacher\\'s pockets, that are not kindly received by the preacher, as he quickly departs from the preacher\\'s shouts that he doesn\\'t want his money.\\r\\nOur perceptions of him, as a Rock of Gibraltor type, is squelched for good, as we see him come unglued in his very comfortable home, as he interacts with his wife (Ellen McLaughlin), his second wife, as she confronts him with a letter from her college-aged daughter, Tracy, who is his daughter via his first marriage and therefore not Ellen\\'s real daughter. Ellen insists on reading a letter addressed to her from Tracy, out loud, accusing him of placing his hands on her private places, as he responds to his wife\\'s question, all she wants to know, is it true? And all he can respond, is that he wonders why Tracy is doing this to him, saying that she is probably mixed up. What results is apocalyptic, as the film becomes disturbingly mysterious and evasive, never settling for sure who is telling the truth, but destroying the family as it is. This scene could also be deemed as an attack on America\\'s soul, exposing it to questions about truth and character, principles that are put under the microscope, as the story builds to its very tragic outcome.\\r\\nThis is one of Jost\\'s deepest and most penetrating films, it could even be argued that he has made a classical film, as it forcefully and subtly tells an American story, replete with unanswered questions about family life that are haunting, that give the film a certain power that makes you think for a long time afterwards what is it about this country that is so raw and violent in nature, that becomes a part of the people\\'s own nature.\\r\\nOne of the scenes that I found most memorable, was when the camera panned the diner where Tom was dining with some co-workers and all we could hear, at first, was the muffled conversations of the patrons, as the camera meticulously panned the diner, until the atmosphere of the place was fully absorbed and we returned to Tom and his conversation, which became clearer, as this scene played out a daily experience most Americans have had but has rarely been captured so exactly on film. This time consuming shot, is not attempted by commercial filmmakers who live in fear of losing their audience in a long non-action shot.This is one of Jost\\'s strong points, his willingness to explore territory others fear to go.\\r\\nJost\\'s film can be justifiably criticized for a few lapses in the story line it didn\\'t clarify more precisely, but more importantly, it should be praised for the poetry it brings to its story when telling about a malaise in the American culture that is difficult to come to grips with, as the American landscape is perceived as so beautiful a sight to behold and the country as so wealthy a place when compared with the rest of the world, as it asks... But, what does this mean if Americans are not a happy people?\\r\\nREVIEWED ON 3/20/99\\r\\nDennis Schwartz: \"Movie Reviews\"\\r\\n=A9 ALL RIGHTS RESERVED DENNIS SCHWARTZ\\r\\n'"]},"execution_count":87,"metadata":{},"output_type":"execute_result"}],"source":["news[25]"]},{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":12847,"status":"ok","timestamp":1696526470094,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"ngB7kF8g11nt"},"outputs":[],"source":["# import the shorttext library for text preprocessing\n","#standard_text_preprocessor_1 under shorttext.utils provides a standar way of text preprocessing, including the following steps:\n","\n","   #1. removing special characters,\n","   #2. removing numerals,\n","   #3. converting all alphabets to lower cases,\n","   #4. removing stop words, and\n","   #5. stemming the words (using Porter stemmer).\n","\n","from shorttext.utils import standard_text_preprocessor_1, DocumentTermMatrix\n","preprocessor = standard_text_preprocessor_1()\n","corpus = [preprocessor(article).split(' ') for article in news]"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1696526473693,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"_Wz8YLx1GYqB","outputId":"80bbd9a7-b3f1-47e6-cbd9-d13c6a781a20"},"outputs":[{"data":{"text/plain":["['bed',\n"," 'sleep',\n"," 'director',\n"," 'jon',\n"," 'jost',\n"," 'cast',\n"," 'rai',\n"," 'weiss',\n"," 'tom',\n"," 'blair',\n"," 'doug',\n"," 'marshal',\n"," 'gaddi',\n"," 'beth',\n"," 'kate',\n"," 'sannella',\n"," 'scott',\n"," 'brad',\n"," 'shelton',\n"," 'mr',\n"," 'weiss',\n"," 'mclaughlin',\n"," '\\r\\ntruli',\n"," 'independ',\n"," 'filmmak',\n"," 'jost',\n"," 'complet',\n"," 'socal',\n"," 'trilogi',\n"," 'rural',\n"," 'america',\n"," 'film',\n"," 'sinc',\n"," 'move',\n"," 'self',\n"," 'impos',\n"," 'exil',\n"," 'go',\n"," 'europ',\n"," 'extraordinari',\n"," 'film',\n"," 'offer',\n"," 'long',\n"," 'hard',\n"," 'look',\n"," 'subject',\n"," 'matter',\n"," 'camera',\n"," 'held',\n"," 'steadfast',\n"," 'move',\n"," 'insati',\n"," 'long',\n"," 'period',\n"," 'time',\n"," 'pick',\n"," 'appropri',\n"," 'nuanc',\n"," 'need',\n"," 'deliber',\n"," 'dispass',\n"," 'look',\n"," 'oregon',\n"," 'lumber',\n"," 'mill',\n"," 'whose',\n"," 'owner',\n"," 'tom',\n"," 'blair',\n"," 'face',\n"," 'unsettl',\n"," 'econom',\n"," 'new',\n"," 'busi',\n"," 'builtup',\n"," 'work',\n"," '',\n"," 'odd',\n"," 'year',\n"," 'life',\n"," 'focus',\n"," 'tri',\n"," 'find',\n"," 'us',\n"," 'metaphor',\n"," 'symbol',\n"," 'america',\n"," 'perhap',\n"," 'emul',\n"," 'emerson',\n"," 'view',\n"," 'write',\n"," 'flash',\n"," 'screen',\n"," 'exhibit',\n"," 'sai',\n"," 'essai',\n"," 'natur',\n"," 'america\\r\\nbi',\n"," 'see',\n"," 'get',\n"," 'see',\n"," 'adjust',\n"," 'carefulli',\n"," 'script',\n"," 'life',\n"," 'flyfish',\n"," 'love',\n"," 'sport',\n"," 'easi',\n"," 'almost',\n"," 'gentil',\n"," 'manner',\n"," 'definit',\n"," 'american',\n"," 'persona',\n"," 'forc',\n"," 'econom',\n"," 'necess',\n"," 'deal',\n"," 'japanes',\n"," 'businessmen',\n"," 'inher',\n"," 'despis',\n"," 'get',\n"," 'pictur',\n"," 'rather',\n"," 'complic',\n"," 'individu',\n"," 'difficulti',\n"," 'commun',\n"," 'other',\n"," 'closer',\n"," 'get',\n"," 'sens',\n"," 'lot',\n"," 'thing',\n"," 'remain',\n"," 'unansw',\n"," 'shocker',\n"," 'life',\n"," 'unfold',\n"," 'come',\n"," 'meet',\n"," 'foreign',\n"," 'stranger',\n"," 'street',\n"," 'rave',\n"," 'dai',\n"," 'aton',\n"," 'come',\n"," 'soon',\n"," 'god',\n"," 'know',\n"," 'prai',\n"," 'told',\n"," 'time',\n"," 'feel',\n"," 'uncomfort',\n"," 'around',\n"," 'religi',\n"," 'zealot',\n"," 'fumbl',\n"," 'around',\n"," 'wad',\n"," 'bill',\n"," 'thrust',\n"," 'dollar',\n"," 'preacher',\n"," 'pocket',\n"," 'kindli',\n"," 'receiv',\n"," 'preacher',\n"," 'quickli',\n"," 'depart',\n"," 'preacher',\n"," 'shout',\n"," 'doesnt',\n"," 'money\\r\\nour',\n"," 'percept',\n"," 'rock',\n"," 'gibraltor',\n"," 'type',\n"," 'squelch',\n"," 'good',\n"," 'see',\n"," 'unglu',\n"," 'comfort',\n"," 'home',\n"," 'interact',\n"," 'wife',\n"," 'mclaughlin',\n"," 'second',\n"," 'wife',\n"," 'confront',\n"," 'letter',\n"," 'collegeag',\n"," 'daughter',\n"," 'traci',\n"," 'daughter',\n"," 'via',\n"," 'first',\n"," 'marriag',\n"," 'therefor',\n"," 'ellen',\n"," 'real',\n"," 'daughter',\n"," 'insist',\n"," 'read',\n"," 'letter',\n"," 'address',\n"," 'traci',\n"," 'loud',\n"," 'accus',\n"," 'place',\n"," 'hand',\n"," 'privat',\n"," 'place',\n"," 'respond',\n"," 'wife',\n"," 'question',\n"," 'want',\n"," 'know',\n"," 'true',\n"," 'respond',\n"," 'wonder',\n"," 'traci',\n"," 'sai',\n"," 'probabl',\n"," 'mix',\n"," 'result',\n"," 'apocalypt',\n"," 'film',\n"," 'becom',\n"," 'disturbingli',\n"," 'mysteri',\n"," 'evas',\n"," 'never',\n"," 'settl',\n"," 'sure',\n"," 'tell',\n"," 'truth',\n"," 'destroi',\n"," 'famili',\n"," 'scene',\n"," 'could',\n"," 'deem',\n"," 'attack',\n"," 'america',\n"," 'soul',\n"," 'expos',\n"," 'question',\n"," 'truth',\n"," 'charact',\n"," 'principl',\n"," 'put',\n"," 'microscop',\n"," 'stori',\n"," 'build',\n"," 'tragic',\n"," 'outcome\\r\\nthi',\n"," 'on',\n"," 'jost',\n"," 'deepest',\n"," 'penetr',\n"," 'film',\n"," 'could',\n"," 'even',\n"," 'argu',\n"," 'made',\n"," 'classic',\n"," 'film',\n"," 'forcefulli',\n"," 'subtli',\n"," 'tell',\n"," 'american',\n"," 'stori',\n"," 'replet',\n"," 'unansw',\n"," 'question',\n"," 'famili',\n"," 'life',\n"," 'haunt',\n"," 'give',\n"," 'film',\n"," 'certain',\n"," 'power',\n"," 'make',\n"," 'think',\n"," 'long',\n"," 'time',\n"," 'afterward',\n"," 'countri',\n"," 'raw',\n"," 'violent',\n"," 'natur',\n"," 'becom',\n"," 'part',\n"," 'peopl',\n"," 'nature\\r\\non',\n"," 'scene',\n"," 'found',\n"," 'memor',\n"," 'camera',\n"," 'pan',\n"," 'diner',\n"," 'tom',\n"," 'dine',\n"," 'cowork',\n"," 'could',\n"," 'hear',\n"," 'first',\n"," 'muffl',\n"," 'convers',\n"," 'patron',\n"," 'camera',\n"," 'meticul',\n"," 'pan',\n"," 'diner',\n"," 'atmospher',\n"," 'place',\n"," 'fulli',\n"," 'absorb',\n"," 'return',\n"," 'tom',\n"," 'convers',\n"," 'becam',\n"," 'clearer',\n"," 'scene',\n"," 'plai',\n"," 'daili',\n"," 'experi',\n"," 'american',\n"," 'rare',\n"," 'captur',\n"," 'exactli',\n"," 'film',\n"," 'time',\n"," 'consum',\n"," 'shot',\n"," 'attempt',\n"," 'commerci',\n"," 'filmmak',\n"," 'live',\n"," 'fear',\n"," 'lose',\n"," 'audienc',\n"," 'long',\n"," 'nonact',\n"," 'shotthi',\n"," 'on',\n"," 'jost',\n"," 'strong',\n"," 'point',\n"," 'willing',\n"," 'explor',\n"," 'territori',\n"," 'other',\n"," 'fear',\n"," 'go\\r\\njost',\n"," 'film',\n"," 'justifi',\n"," 'critic',\n"," 'laps',\n"," 'stori',\n"," 'line',\n"," 'didnt',\n"," 'clarifi',\n"," 'precis',\n"," 'importantli',\n"," 'prais',\n"," 'poetri',\n"," 'bring',\n"," 'stori',\n"," 'tell',\n"," 'malais',\n"," 'american',\n"," 'cultur',\n"," 'difficult',\n"," 'grip',\n"," 'american',\n"," 'landscap',\n"," 'perceiv',\n"," 'beauti',\n"," 'sight',\n"," 'behold',\n"," 'countri',\n"," 'wealthi',\n"," 'place',\n"," 'compar',\n"," 'rest',\n"," 'world',\n"," 'ask',\n"," 'mean',\n"," 'american',\n"," 'happi',\n"," 'people\\r\\nreview',\n"," '\\r\\ndenni',\n"," 'schwartz',\n"," 'movi',\n"," 'reviews\\r\\na',\n"," 'right',\n"," 'reserv',\n"," 'denni',\n"," 'schwartz\\r\\n']"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["corpus[25]"]},{"cell_type":"code","execution_count":90,"metadata":{"executionInfo":{"elapsed":787,"status":"ok","timestamp":1696526519259,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"9Q3gIccD11nu"},"outputs":[],"source":["#convert the corpus of news to a document term matrix (dtm), where each row represents a document, each column represents the number of occurence for a word\n","\n","dtm = DocumentTermMatrix(corpus, docids = filelists.fileids())"]},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":340,"status":"ok","timestamp":1696526540501,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"CIxeUJC311nx","outputId":"c9266336-2f0a-4ec5-b98f-b64b51bf6223"},"outputs":[{"data":{"text/plain":["dict"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["type(dtm.get_token_occurences('director'))"]},{"cell_type":"code","execution_count":92,"metadata":{"executionInfo":{"elapsed":137,"status":"ok","timestamp":1696526571985,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"YbJ8iXsx11nx"},"outputs":[],"source":["# define a function to calculate entropy given a distribution\n","import numpy as np\n","import math\n","\n","def entropy(p):\n","    if sum(p) == 0:\n","        return 0\n","\n","    p = p/sum(p)\n","\n","    p = p[ p > 0 ]\n","\n","    H = -sum(p*np.log2(p))\n","\n","    return H"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":120,"status":"ok","timestamp":1696526580900,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"H9jP42CD687a","outputId":"58d0ddc6-46f1-48d4-d7d0-c92942febff8"},"outputs":[],"source":["director_counts: list = list(dtm.get_token_occurences('director').values())\n","director_docs: list = list(dtm.get_token_occurences('director').keys())"]},{"cell_type":"code","execution_count":94,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Entropy of `director` in the documents: 7.122166395775948\n"]}],"source":["print('Entropy of `director` in the documents:', entropy(director_counts))"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["# of Berardinelli docs having `director`: 77\n","# of Schwartz docs having `director`: 85\n"]}],"source":["bern_doc_count: int = sum([1 for doc in director_docs if doc in bern_files])\n","bern_no_doc_count: int = bern_file_count - bern_doc_count\n","print('# of Berardinelli docs having `director`:', bern_doc_count)\n","\n","sch_doc_count: int = sum([1 for doc in director_docs if doc in sch_files])\n","sch_no_doc_count: int = sch_file_count - sch_doc_count\n","print('# of Schwartz docs having `director`:', sch_doc_count)"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":142,"status":"ok","timestamp":1696526607442,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"yaYaXFq_11n4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matrix of counts: [[77  3]\n"," [85 15]]\n"]}],"source":["#this is an important step\n","array = np.reshape((bern_doc_count, bern_no_doc_count, sch_doc_count, sch_no_doc_count), (2,2))\n","print('Matrix of counts:', array)"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":182,"status":"ok","timestamp":1696526620008,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"},"user_tz":240},"id":"7-SCYZln11n4","outputId":"31528843-95e0-4fa8-d26a-e69efc69de07"},"outputs":[{"data":{"text/plain":["array([[77,  3],\n","       [85, 15]])"]},"execution_count":97,"metadata":{},"output_type":"execute_result"}],"source":["array"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Marginal entropy: 0.9910760598382222\n"]}],"source":["marginal_entropy = entropy(np.sum(array, axis=1))\n","print('Marginal entropy:', marginal_entropy)"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Column probabilities: [0.9 0.1]\n"]}],"source":["column_prob = np.sum(array, axis=0)/total_file_count\n","print('Column probabilities:', column_prob)"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Column entropy: [0.99824017 0.65002242]\n","Conditional entropy: 0.9634183936209307\n"]}],"source":["column_entropy = np.apply_along_axis(entropy, 0, array)\n","print('Column entropy:', column_entropy)\n","\n","conditional_entropy = sum(column_prob * column_entropy)\n","print('Conditional entropy:', conditional_entropy)"]},{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Mutual information: 0.027657666217291488\n"]}],"source":["mi = marginal_entropy - conditional_entropy\n","print('Mutual information:', mi)"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Entropy of `director` in the documents: 7.122166395775948\n","# of Berardinelli docs having `director`: 77\n","# of Schwartz docs having `director`: 85\n","Matrix of counts: [[77  3]\n"," [85 15]]\n","Marginal entropy: 0.9910760598382222\n","Column probabilities: [0.9 0.1]\n","Column entropy: [0.99824017 0.65002242]\n","Conditional entropy: 0.9634183936209307\n","Mutual information: 0.027657666217291488\n"]}],"source":["director_counts: list = list(dtm.get_token_occurences('director').values())\n","director_docs: list = list(dtm.get_token_occurences('director').keys())\n","\n","print('Entropy of `director` in the documents:', entropy(director_counts))\n","\n","bern_doc_count: int = sum([1 for doc in director_docs if doc in bern_files])\n","bern_no_doc_count: int = bern_file_count - bern_doc_count\n","print('# of Berardinelli docs having `director`:', bern_doc_count)\n","\n","sch_doc_count: int = sum([1 for doc in director_docs if doc in sch_files])\n","sch_no_doc_count: int = sch_file_count - sch_doc_count\n","print('# of Schwartz docs having `director`:', sch_doc_count)\n","\n","array = np.reshape((bern_doc_count, bern_no_doc_count, sch_doc_count, sch_no_doc_count), (2,2))\n","print('Matrix of counts:', array)\n","\n","array\n","\n","marginal_entropy = entropy(np.sum(array, axis=1))\n","print('Marginal entropy:', marginal_entropy)\n","\n","column_prob = np.sum(array, axis=0)/total_file_count\n","print('Column probabilities:', column_prob)\n","\n","column_entropy = np.apply_along_axis(entropy, 0, array)\n","print('Column entropy:', column_entropy)\n","\n","conditional_entropy = sum(column_prob * column_entropy)\n","print('Conditional entropy:', conditional_entropy)\n","\n","mi = marginal_entropy - conditional_entropy\n","print('Mutual information:', mi)"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["def calculate_word_association(word):\n","    director_counts = list(dtm.get_token_occurences(word).values())\n","    director_docs = list(dtm.get_token_occurences(word).keys())\n","\n","    print('Entropy of `{}` in the documents:'.format(word), entropy(director_counts))\n","\n","    bern_doc_count = sum([1 for doc in director_docs if doc in bern_files])\n","    bern_no_doc_count = bern_file_count - bern_doc_count\n","    print('# of Berardinelli docs having `{}`:'.format(word), bern_doc_count)\n","\n","    sch_doc_count = sum([1 for doc in director_docs if doc in sch_files])\n","    sch_no_doc_count = sch_file_count - sch_doc_count\n","    print('# of Schwartz docs having `{}`:'.format(word), sch_doc_count)\n","\n","    array = np.reshape((bern_doc_count, bern_no_doc_count, sch_doc_count, sch_no_doc_count), (2,2))\n","    print('Matrix of counts:', array)\n","\n","    marginal_entropy = entropy(np.sum(array, axis=1))\n","    print('Marginal entropy:', marginal_entropy)\n","\n","    column_prob = np.sum(array, axis=0)/total_file_count\n","    print('Column probabilities:', column_prob)\n","\n","    column_entropy = np.apply_along_axis(entropy, 0, array)\n","    print('Column entropy:', column_entropy)\n","\n","    conditional_entropy = sum(column_prob * column_entropy)\n","    print('Conditional entropy:', conditional_entropy)\n","\n","    mi = marginal_entropy - conditional_entropy\n","    print('Mutual information:', mi)"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Entropy of `director` in the documents: 7.122166395775948\n","# of Berardinelli docs having `director`: 77\n","# of Schwartz docs having `director`: 85\n","Matrix of counts: [[77  3]\n"," [85 15]]\n","Marginal entropy: 0.9910760598382222\n","Column probabilities: [0.9 0.1]\n","Column entropy: [0.99824017 0.65002242]\n","Conditional entropy: 0.9634183936209307\n","Mutual information: 0.027657666217291488\n"]}],"source":["test = calculate_word_association('director')\n","test"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["def calculate_word_MI(word):\n","    director_counts = list(dtm.get_token_occurences(word).values())\n","    director_docs = list(dtm.get_token_occurences(word).keys())\n","\n","    print('Entropy of `{}` in the documents:'.format(word), entropy(director_counts))\n","\n","    bern_doc_count = sum([1 for doc in director_docs if doc in bern_files])\n","    bern_no_doc_count = bern_file_count - bern_doc_count\n","\n","    sch_doc_count = sum([1 for doc in director_docs if doc in sch_files])\n","    sch_no_doc_count = sch_file_count - sch_doc_count\n","\n","    array = np.reshape((bern_doc_count, bern_no_doc_count, sch_doc_count, sch_no_doc_count), (2,2))\n","\n","    marginal_entropy = entropy(np.sum(array, axis=1))\n","\n","    column_prob = np.sum(array, axis=0)/total_file_count\n","\n","    column_entropy = np.apply_along_axis(entropy, 0, array)\n","\n","    conditional_entropy = sum(column_prob * column_entropy)\n","\n","    mi = marginal_entropy - conditional_entropy\n","    print('Mutual information:', mi)"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["vocabulary = {}\n","for document in corpus:\n","    for word in document:\n","        if word not in vocabulary:\n","            vocabulary[word] = 1\n","        else:\n","            vocabulary[word] += 1\n"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Top ten words:\n","reserv: 0.5360547127375266\n","denni: 0.5177766824991015\n","schwartz\n",": 0.49202296056300543\n","right: 0.34124798879941887\n","there: 0.29684962309392304\n","howev: 0.27421790253727896\n","releas: 0.2702108404097968\n","cast: 0.2564686677667559\n","screenplai: 0.2564686677667559\n","produc: 0.23020448708340835\n"]}],"source":["def calculate_word_MI(word):\n","    director_counts = list(dtm.get_token_occurences(word).values())\n","    director_docs = list(dtm.get_token_occurences(word).keys())\n","\n","    entropy_value = entropy(director_counts)\n","\n","    bern_doc_count = sum([1 for doc in director_docs if doc in bern_files])\n","    bern_no_doc_count = bern_file_count - bern_doc_count\n","\n","    sch_doc_count = sum([1 for doc in director_docs if doc in sch_files])\n","    sch_no_doc_count = sch_file_count - sch_doc_count\n","\n","    array = np.reshape((bern_doc_count, bern_no_doc_count, sch_doc_count, sch_no_doc_count), (2,2))\n","\n","    marginal_entropy = entropy(np.sum(array, axis=1))\n","\n","    column_prob = np.sum(array, axis=0)/total_file_count\n","\n","    column_entropy = np.apply_along_axis(entropy, 0, array)\n","\n","    conditional_entropy = sum(column_prob * column_entropy)\n","\n","    mi = marginal_entropy - conditional_entropy\n","    return mi\n","    \n","    \n","# Create an empty dictionary to store the MI scores for each word\n","mi_scores = {}\n","\n","# Iterate over each word in the vocabulary\n","for word in vocabulary.keys():\n","    # Calculate the MI score for the word using the calculate_word_MI function\n","    mi_score = calculate_word_MI(word)\n","    # Store the MI score in the dictionary\n","    mi_scores[word] = mi_score\n","\n","# Sort the dictionary in descending order based on the MI scores\n","sorted_words = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","# Get the top ten words\n","top_ten_words = [word for word, _ in sorted_words[:10]]\n","\n","# Sort the dictionary in descending order based on the MI scores\n","sorted_words = sorted(mi_scores.items(), key=lambda x: x[1], reverse=True)\n","\n","# Print the top ten words with their mutual information\n","print(\"Top ten words:\")\n","for word, mi_score in sorted_words[:10]:\n","    print(f\"{word}: {mi_score}\")\n","\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"}},"nbformat":4,"nbformat_minor":0}
