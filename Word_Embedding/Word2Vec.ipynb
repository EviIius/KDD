{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMIMyEp9qcCpSdkeHviwfL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"kyJF50a9P31g","executionInfo":{"status":"ok","timestamp":1712062157136,"user_tz":240,"elapsed":4900,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"outputs":[],"source":["import io\n","import re\n","import string\n","import tqdm\n","\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras import layers"]},{"cell_type":"code","source":["sentence = \"The wide road shimmered in the hot sun\"\n","tokens = list(sentence.lower().split())\n","print(len(tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gQrqJoQbQA0s","executionInfo":{"status":"ok","timestamp":1712062159199,"user_tz":240,"elapsed":218,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"1f8252ec-63d8-466f-e055-83aab05e6936"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["8\n"]}]},{"cell_type":"code","source":["vocab, index = {}, 1  # start indexing from 1\n","vocab['<pad>'] = 0  # add a padding token\n","for token in tokens:\n","  if token not in vocab:\n","    vocab[token] = index\n","    index += 1\n","vocab_size = len(vocab)\n","print(vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sy_4v1YqQXy3","executionInfo":{"status":"ok","timestamp":1712062160948,"user_tz":240,"elapsed":168,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"ae85e7c6-26a4-4206-f53d-4a813c6551f7"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n"]}]},{"cell_type":"code","source":["inverse_vocab = {index: token for token, index in vocab.items()}\n","print(inverse_vocab)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XWIUNrwWQjEG","executionInfo":{"status":"ok","timestamp":1712062164655,"user_tz":240,"elapsed":208,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"6554b2ac-1881-4113-c689-a37f84c2449d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n"]}]},{"cell_type":"code","source":["example_sequence = [vocab[word] for word in tokens]\n","print(example_sequence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gbdrrQ12RBZg","executionInfo":{"status":"ok","timestamp":1712062166764,"user_tz":240,"elapsed":227,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"f93372f8-59c6-4c42-c861-8d9cb5520124"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5, 1, 6, 7]\n"]}]},{"cell_type":"code","source":["window_size = 2\n","positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(         #\",_\"means ignoring the last output\n","      example_sequence,\n","      vocabulary_size=vocab_size,\n","      window_size=window_size,\n","      negative_samples=0)"],"metadata":{"id":"l_ky43qVRNww","executionInfo":{"status":"ok","timestamp":1712062168616,"user_tz":240,"elapsed":178,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["positive_skip_grams"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wh-nzNbRYR2","executionInfo":{"status":"ok","timestamp":1712062169781,"user_tz":240,"elapsed":228,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"9cca9672-d3af-4c0b-a8e2-44741e864234"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[1, 3],\n"," [2, 1],\n"," [3, 2],\n"," [6, 1],\n"," [2, 3],\n"," [5, 3],\n"," [1, 7],\n"," [7, 1],\n"," [6, 5],\n"," [5, 1],\n"," [3, 1],\n"," [4, 5],\n"," [3, 5],\n"," [5, 4],\n"," [1, 2],\n"," [1, 4],\n"," [7, 6],\n"," [3, 4],\n"," [6, 7],\n"," [1, 6],\n"," [1, 5],\n"," [4, 1],\n"," [4, 3],\n"," [5, 6],\n"," [4, 2],\n"," [2, 4]]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["for target, context in positive_skip_grams:\n","  print(f\"({target}, {context}): ({inverse_vocab[target]}, {inverse_vocab[context]})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vBo7IHuRfDh","executionInfo":{"status":"ok","timestamp":1712062171458,"user_tz":240,"elapsed":174,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"e824b674-dff8-4b59-884d-c8c03c35e9d1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["(1, 3): (the, road)\n","(2, 1): (wide, the)\n","(3, 2): (road, wide)\n","(6, 1): (hot, the)\n","(2, 3): (wide, road)\n","(5, 3): (in, road)\n","(1, 7): (the, sun)\n","(7, 1): (sun, the)\n","(6, 5): (hot, in)\n","(5, 1): (in, the)\n","(3, 1): (road, the)\n","(4, 5): (shimmered, in)\n","(3, 5): (road, in)\n","(5, 4): (in, shimmered)\n","(1, 2): (the, wide)\n","(1, 4): (the, shimmered)\n","(7, 6): (sun, hot)\n","(3, 4): (road, shimmered)\n","(6, 7): (hot, sun)\n","(1, 6): (the, hot)\n","(1, 5): (the, in)\n","(4, 1): (shimmered, the)\n","(4, 3): (shimmered, road)\n","(5, 6): (in, hot)\n","(4, 2): (shimmered, wide)\n","(2, 4): (wide, shimmered)\n"]}]},{"cell_type":"code","source":["_ #all postive skip grams, therefore the result will be all 1."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbGFq9bqR6Wz","executionInfo":{"status":"ok","timestamp":1712062173801,"user_tz":240,"elapsed":185,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"41fe9472-8a49-49eb-8463-976db6642dbd"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Get target and context words of the first  positive skip-gram.\n","target_word, context_word = positive_skip_grams[0]\n","\n","# Pass the context word as true class to exclude it from being sampled for negative sampling\n","context_class = tf.reshape(tf.constant(context_word, dtype=\"int64\"), (1, 1))"],"metadata":{"id":"Hey47c8FT1cL","executionInfo":{"status":"ok","timestamp":1712062175706,"user_tz":240,"elapsed":237,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["context_class"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okyqO8XrUFp6","executionInfo":{"status":"ok","timestamp":1712062178602,"user_tz":240,"elapsed":226,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"e92a5757-860d-4b1b-addd-7a51846c6337"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[3]])>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["# Set the number of negative samples per positive context.\n","num_ns = 4\n","negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","    true_classes=context_class,  # class that should be sampled as 'positive'\n","    num_true=1,  # each positive skip-gram has 1 positive context class\n","    num_sampled=num_ns,  # number of negative context words to sample\n","    unique=True,  # all the negative samples should be unique\n","    range_max=vocab_size,  # pick index of the samples from [0, vocab_size]\n","    seed = 42,  # seed for reproducibility\n","    name=\"negative_sampling\"  # name of this operation\n",")"],"metadata":{"id":"GUtGiwMJTsYX","executionInfo":{"status":"ok","timestamp":1712062179477,"user_tz":240,"elapsed":142,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["negative_sampling_candidates"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"03_X8upPZnSh","executionInfo":{"status":"ok","timestamp":1712062181236,"user_tz":240,"elapsed":2,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"5f4505a1-a3fe-42e6-a9bb-161c8f1889f2"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(4,), dtype=int64, numpy=array([2, 1, 4, 3])>"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["print([inverse_vocab[index.numpy()] for index in negative_sampling_candidates])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSWyUIr8Z-Jo","executionInfo":{"status":"ok","timestamp":1712062183897,"user_tz":240,"elapsed":287,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"cd1122ab-a2de-435c-c52b-43cb0f389285"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['wide', 'the', 'shimmered', 'road']\n"]}]},{"cell_type":"code","source":["# Reduce a dimension so you can use concatenation (in the next step).\n","squeezed_context_class = tf.squeeze(context_class, 1)\n","\n","# Concatenate a positive context word with negative sampled words.\n","context = tf.concat([squeezed_context_class, negative_sampling_candidates], 0)\n","\n","context"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J_Te_HIscLDO","executionInfo":{"status":"ok","timestamp":1712062206811,"user_tz":240,"elapsed":164,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"203556ec-36ee-467a-88df-c45bba979726"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5,), dtype=int64, numpy=array([3, 2, 1, 4, 3])>"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# Label the first context word as `1` (positive) followed by `num_ns` `0`s (negative).\n","label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ia6yiNFBcrKN","executionInfo":{"status":"ok","timestamp":1712062208688,"user_tz":240,"elapsed":240,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"e43fc85e-6c11-415b-9c16-dc05582b31f0"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 0, 0, 0, 0])>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["A tuple of (target_word, context, label) tensors constitutes one training example for training the skip-gram negative sampling word2vec model. Notice that the target is of shape (1,) while the context and label are of shape (1+num_ns,)"],"metadata":{"id":"JOm3qzAJedNS"}},{"cell_type":"code","source":["#You can use the tf.keras.preprocessing.sequence.make_sampling_table to generate a word-frequency rank based probabilistic sampling table and pass it to the skipgrams function. Inspect the sampling probabilities for a vocab_size of 10.\n","sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(size=10)\n","print(sampling_table)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6F6ybYWNdFdt","executionInfo":{"status":"ok","timestamp":1712062210718,"user_tz":240,"elapsed":196,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"9ea802a3-f061-4228-eef8-f2a4e142636a"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["[0.00315225 0.00315225 0.00547597 0.00741556 0.00912817 0.01068435\n"," 0.01212381 0.01347162 0.01474487 0.0159558 ]\n"]}]},{"cell_type":"markdown","source":["sampling_table[i] denotes the probability of sampling the i-th most common word in a dataset. The function assumes a Zipf's distribution of the word frequencies for sampling."],"metadata":{"id":"QINq_WxIeWf8"}},{"cell_type":"markdown","source":["**Generate training data**\n","\n","Compile all the steps described above into a function that can be called on a list of vectorized sentences obtained from any text dataset. Notice that the sampling table is built before sampling skip-gram word pairs. You will use this function in the later sections."],"metadata":{"id":"ECuq2rqNewTO"}},{"cell_type":"code","source":["# Generates skip-gram pairs with negative sampling for a list of sequences\n","# (int-encoded sentences) based on window size, number of negative samples\n","# and vocabulary size.\n","def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n","  # Elements of each training example are appended to these lists.\n","  target_words, contexts, labels = [], [], []\n","\n","  # Build the sampling table for `vocab_size` tokens.\n","  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n","\n","  # Iterate over all sequences (sentences) in the dataset.\n","  for sequence in tqdm.tqdm(sequences):\n","\n","    # Generate positive skip-gram pairs for a sequence (sentence).\n","    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n","          sequence,\n","          vocabulary_size=vocab_size,\n","          sampling_table=sampling_table,\n","          window_size=window_size,\n","          negative_samples=0)\n","\n","    # Iterate over each positive skip-gram pair to produce training examples\n","    # with a positive context word and negative samples.\n","    for target_word, context_word in positive_skip_grams:\n","      context_class = tf.expand_dims(\n","          tf.constant([context_word], dtype=\"int64\"), 1)\n","      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n","          true_classes=context_class,\n","          num_true=1,\n","          num_sampled=num_ns,\n","          unique=True,\n","          range_max=vocab_size,\n","          seed=seed,\n","          name=\"negative_sampling\")\n","\n","      # Build context and label vectors (for one target word)\n","      context = tf.concat([tf.squeeze(context_class,1), negative_sampling_candidates], 0)\n","      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n","\n","      # Append each element from the training example to global lists.\n","      target_words.append(target_word)\n","      contexts.append(context)\n","      labels.append(label)\n","\n","  return target_words, contexts, labels"],"metadata":{"id":"2IjjYt5Ze1VL","executionInfo":{"status":"ok","timestamp":1712062217789,"user_tz":240,"elapsed":169,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2JN_gZCCfuqB","executionInfo":{"status":"ok","timestamp":1712062221471,"user_tz":240,"elapsed":341,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"311e6826-ba21-4ba8-83f4-0d7281575252"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n","1115394/1115394 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","source":["with open(path_to_file) as f:\n","  lines = f.read().splitlines()\n","for line in lines[:20]:\n","  print(line)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oPL11JNcf39C","executionInfo":{"status":"ok","timestamp":1712062222172,"user_tz":240,"elapsed":161,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"7a4151a6-eccb-4426-abdd-e41e067ea71e"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["First Citizen:\n","Before we proceed any further, hear me speak.\n","\n","All:\n","Speak, speak.\n","\n","First Citizen:\n","You are all resolved rather to die than to famish?\n","\n","All:\n","Resolved. resolved.\n","\n","First Citizen:\n","First, you know Caius Marcius is chief enemy to the people.\n","\n","All:\n","We know't, we know't.\n","\n","First Citizen:\n","Let us kill him, and we'll have corn at our own price.\n"]}]},{"cell_type":"code","source":["#remove\n","text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"],"metadata":{"id":"AgtGfk55gA7q","executionInfo":{"status":"ok","timestamp":1712062683654,"user_tz":240,"elapsed":337,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# Now, create a custom standardization function to lowercase the text and\n","# remove punctuation.\n","def custom_standardization(input_data):\n","  lowercase = tf.strings.lower(input_data)\n","  return tf.strings.regex_replace(lowercase,\n","                                  '[%s]' % re.escape(string.punctuation), '')\n","\n","\n","# Define the vocabulary size and the number of words in a sequence.\n","vocab_size = 4096\n","sequence_length = 10\n","\n","# Use the `TextVectorization` layer to normalize, split, and map text strings to\n","# integers. Set the `output_sequence_length` length to pad all samples to the\n","# same length.\n","vectorize_layer = layers.TextVectorization(\n","    standardize=custom_standardization,\n","    max_tokens=vocab_size,\n","    output_mode='int',\n","    output_sequence_length=sequence_length)"],"metadata":{"id":"J6nIX6fwgtv1","executionInfo":{"status":"ok","timestamp":1712062685092,"user_tz":240,"elapsed":205,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["vectorize_layer.adapt(text_ds.batch(1024))"],"metadata":{"id":"C834r8pjhS9-","executionInfo":{"status":"ok","timestamp":1712062687870,"user_tz":240,"elapsed":1584,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Save the created vocabulary for reference.\n","inverse_vocab = vectorize_layer.get_vocabulary()\n","print(inverse_vocab[:20])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCjkeuOehZg1","executionInfo":{"status":"ok","timestamp":1712062701978,"user_tz":240,"elapsed":252,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"b1c10353-223e-400f-f996-303e5508e107"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["['', '[UNK]', 'the', 'and', 'to', 'i', 'of', 'you', 'my', 'a', 'that', 'in', 'is', 'not', 'for', 'with', 'me', 'it', 'be', 'your']\n"]}]},{"cell_type":"code","source":["# Vectorize the data in text_ds.\n","AUTOTUNE = tf.data.AUTOTUNE\n","\n","text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"],"metadata":{"id":"ODdPZVF25HLo","executionInfo":{"status":"ok","timestamp":1712062806697,"user_tz":240,"elapsed":198,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["sequences = list(text_vector_ds.as_numpy_iterator())\n","print(len(sequences))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d4wfSd2s5niz","executionInfo":{"status":"ok","timestamp":1712062898366,"user_tz":240,"elapsed":7796,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"c6ef9520-4e6b-4c46-b4f5-c79fb64cce90"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["32777\n"]}]},{"cell_type":"code","source":["for seq in sequences[:5]:\n","  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_oEbapsM5u8o","executionInfo":{"status":"ok","timestamp":1712062928586,"user_tz":240,"elapsed":205,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"0fbfcad1-f97e-40d4-b90c-9926a1f8a64b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n","[138  36 982 144 673 125  16 106   0   0] => ['before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', '', '']\n","[34  0  0  0  0  0  0  0  0  0] => ['all', '', '', '', '', '', '', '', '', '']\n","[106 106   0   0   0   0   0   0   0   0] => ['speak', 'speak', '', '', '', '', '', '', '', '']\n","[ 89 270   0   0   0   0   0   0   0   0] => ['first', 'citizen', '', '', '', '', '', '', '', '']\n"]}]},{"cell_type":"code","source":["targets, contexts, labels = generate_training_data(\n","    sequences=sequences,\n","    window_size=2,\n","    num_ns=4,\n","    vocab_size=vocab_size,\n","    seed=42)\n","\n","targets = np.array(targets)\n","contexts = np.array(contexts)\n","labels = np.array(labels)\n","\n","print('\\n')\n","print(f\"targets.shape: {targets.shape}\")\n","print(f\"contexts.shape: {contexts.shape}\")\n","print(f\"labels.shape: {labels.shape}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82-7LjNu5_DG","executionInfo":{"status":"ok","timestamp":1712063025574,"user_tz":240,"elapsed":25972,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"c9aece2e-5845-4ed2-b9c9-68aa767db458"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 32777/32777 [00:24<00:00, 1361.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","\n","targets.shape: (65114,)\n","contexts.shape: (65114, 5)\n","labels.shape: (65114, 5)\n"]}]},{"cell_type":"code","source":["BATCH_SIZE = 1024\n","BUFFER_SIZE = 10000\n","dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n","print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnBBVwhd6aPZ","executionInfo":{"status":"ok","timestamp":1712063099150,"user_tz":240,"elapsed":208,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"9337d2d5-796e-4c8d-8adf-ca6ae6486b77"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["<_BatchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"]}]},{"cell_type":"code","source":["dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n","print(dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lFV1YyMa6cBw","executionInfo":{"status":"ok","timestamp":1712063116294,"user_tz":240,"elapsed":190,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"cb1418bd-a8d9-4ad3-ea12-d5c172599e7f"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["<_PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"]}]},{"cell_type":"code","source":["class Word2Vec(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim):\n","    super(Word2Vec, self).__init__()\n","    self.target_embedding = layers.Embedding(vocab_size,\n","                                      embedding_dim,\n","                                      name=\"w2v_embedding\")\n","    self.context_embedding = layers.Embedding(vocab_size,\n","                                       embedding_dim)\n","\n","  def call(self, pair):\n","    target, context = pair\n","\n","    if len(target.shape) == 2:\n","      target = tf.squeeze(target, axis=1)\n","    word_emb = self.target_embedding(target)\n","    context_emb = self.context_embedding(context)\n","    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n","    return dots"],"metadata":{"id":"sQieZayc6uu6","executionInfo":{"status":"ok","timestamp":1712063338253,"user_tz":240,"elapsed":148,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["embedding_dim = 128\n","word2vec = Word2Vec(vocab_size, embedding_dim)\n","word2vec.compile(optimizer='adam',\n","                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","                 metrics=['accuracy'])"],"metadata":{"id":"fhFhMGpW7bsw","executionInfo":{"status":"ok","timestamp":1712063366710,"user_tz":240,"elapsed":183,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["word2vec.fit(dataset, epochs=20)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6eOYuHTC7lR7","executionInfo":{"status":"ok","timestamp":1712063438305,"user_tz":240,"elapsed":28745,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"74254bd5-5440-4702-94c3-e0df622cf864"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","63/63 [==============================] - 2s 22ms/step - loss: 1.6082 - accuracy: 0.2347\n","Epoch 2/20\n","63/63 [==============================] - 1s 17ms/step - loss: 1.5888 - accuracy: 0.5610\n","Epoch 3/20\n","63/63 [==============================] - 1s 16ms/step - loss: 1.5410 - accuracy: 0.6074\n","Epoch 4/20\n","63/63 [==============================] - 1s 17ms/step - loss: 1.4576 - accuracy: 0.5792\n","Epoch 5/20\n","63/63 [==============================] - 1s 16ms/step - loss: 1.3581 - accuracy: 0.5858\n","Epoch 6/20\n","63/63 [==============================] - 1s 16ms/step - loss: 1.2596 - accuracy: 0.6117\n","Epoch 7/20\n","63/63 [==============================] - 1s 16ms/step - loss: 1.1679 - accuracy: 0.6457\n","Epoch 8/20\n","63/63 [==============================] - 1s 23ms/step - loss: 1.0835 - accuracy: 0.6806\n","Epoch 9/20\n","63/63 [==============================] - 1s 17ms/step - loss: 1.0055 - accuracy: 0.7134\n","Epoch 10/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.9334 - accuracy: 0.7423\n","Epoch 11/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.8668 - accuracy: 0.7669\n","Epoch 12/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.8055 - accuracy: 0.7885\n","Epoch 13/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.7490 - accuracy: 0.8068\n","Epoch 14/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.6973 - accuracy: 0.8233\n","Epoch 15/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.6500 - accuracy: 0.8382\n","Epoch 16/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.6068 - accuracy: 0.8520\n","Epoch 17/20\n","63/63 [==============================] - 1s 20ms/step - loss: 0.5675 - accuracy: 0.8643\n","Epoch 18/20\n","63/63 [==============================] - 1s 23ms/step - loss: 0.5316 - accuracy: 0.8750\n","Epoch 19/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.4990 - accuracy: 0.8854\n","Epoch 20/20\n","63/63 [==============================] - 1s 16ms/step - loss: 0.4692 - accuracy: 0.8940\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x795d2029f8b0>"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n","vocab = vectorize_layer.get_vocabulary()"],"metadata":{"id":"C6Fpu8H97vdP","executionInfo":{"status":"ok","timestamp":1712063544785,"user_tz":240,"elapsed":202,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n","out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n","\n","for index, word in enumerate(vocab):\n","  if index == 0:\n","    continue  # skip 0, it's padding.\n","  vec = weights[index]\n","  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n","  out_m.write(word + \"\\n\")\n","out_v.close()\n","out_m.close()"],"metadata":{"id":"suVLCnpM75cA","executionInfo":{"status":"ok","timestamp":1712063546386,"user_tz":240,"elapsed":599,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["try:\n","  from google.colab import files\n","  files.download('vectors.tsv')\n","  files.download('metadata.tsv')\n","except Exception:\n","  pass"],"metadata":{"id":"RKgwNxNt8Ma4","executionInfo":{"status":"ok","timestamp":1712063569421,"user_tz":240,"elapsed":183,"user":{"displayName":"Xi (Sunshine) Niu","userId":"12843704237248706596"}},"outputId":"fbcf26c4-19f1-42aa-c5a5-60706bc76c56","colab":{"base_uri":"https://localhost:8080/","height":17}},"execution_count":39,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_0303b091-df44-4e87-a7db-1b084afd377f\", \"vectors.tsv\", 6118546)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_27995971-9ffd-4043-b616-b4465cb6f47f\", \"metadata.tsv\", 28737)"]},"metadata":{}}]}]}